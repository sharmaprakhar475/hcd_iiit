<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        
    <title>Research Publications | Human-Centered Design @ IIIT-Delhi</title>
    <meta name="title" property="title" content="Research Publications | Human-Centered Design @ IIIT-Delhi" />
    <meta name="og:title" property="og:title" content="Research Publications | Human-Centered Design @ IIIT-Delhi" />
    <meta name="twitter:title" property="twitter:title" content="Research Publications | Human-Centered Design @ IIIT-Delhi" />
    <meta name="description" property="description" content="Welcome to IIIT Delhi’s Department of Human-Centered Design. With an increasing focus on user experience, the importance of Human-Computer Interaction (HCI), Interaction Design, and Design Thinking are also increasing rapidly in IT-based products and services. Digital media is at the heart of many aspects of design in areas like social media, gaming, virtual/augmented reality, etc." />
    <meta name="og:description" property="og:description" content="Welcome to IIIT Delhi’s Department of Human-Centered Design. With an increasing focus on user experience, the importance of Human-Computer Interaction (HCI), Interaction Design, and Design Thinking are also increasing rapidly in IT-based products and services. Digital media is at the heart of many aspects of design in areas like social media, gaming, virtual/augmented reality, etc." />
    <meta name="twitter:description" property="twitter:description" content="Welcome to IIIT Delhi’s Department of Human-Centered Design. With an increasing focus on user experience, the importance of Human-Computer Interaction (HCI), Interaction Design, and Design Thinking are also increasing rapidly in IT-based products and services. Digital media is at the heart of many aspects of design in areas like social media, gaming, virtual/augmented reality, etc." />
    <meta name="og:url" property="og:url" content="https://hcd.iiitd.ac.in/research-publications" />
    <meta name="twitter:url" property="twitter:url" content="https://hcd.iiitd.ac.in/research-publications" />

        <meta property="og:locale" content="en_US" />
        <meta property="og:type" content="website" />
        <meta property="fb:app_id" content="253706309095809" />
        <meta property="og:site_name" content="Department of Human-Centered Design, IIIT-Delhi" />
        <link rel="icon" href="../static/images/icon-96.png" sizes="96x96" />
        <link rel="apple-touch-icon-precomposed" href="../static/images/apple-180.svg" />
        <meta name="google-site-verification" content="7AFyHB1J8USPbEllVeNd5PoN55C-7uhBo8bOe-O797k" />
        <script type="application/ld+json">
            {
              "@id": "https://hcd.iiitd.ac.in/",
              "@context" : "http://schema.org",
              "@type" : "Website",
              "name" : "Department of Human-Centered Design, IIIT-Delhi",
              "description": "Welcome to IIIT Delhi’s Department of Human-Centered Design. With an increasing focus on user experience, the importance of Human-Computer Interaction (HCI), Interaction Design, and Design Thinking are also increasing rapidly in IT-based products and services. Digital media is at the heart of many aspects of design in areas like social media, gaming, virtual/augmented reality, etc.",
              "url" : "https://hcd.iiitd.ac.in/",
              "logo": "https://hcd.iiitd.ac.in/static/images/logo-white.svg",
              "sameAs" : ["https://www.facebook.com/hcdzine","https://twitter.com/hcd_IIITD","https://www.linkedin.com/in/human-centered-design-hcd-43b3a7180/"]
            }
        </script>
        <meta name="theme-color" content="#40ADA7">
        <link rel="manifest" href="/static/manifest.json">
        <link rel="apple-touch-icon" href="../static/images/icon-96.png">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black">
        <meta name="format-detection" content="telephone=no">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
        <link rel="preconnect" href="https://maps.googleapis.com" crossorigin>
        <link rel="preconnect" href="https://maps.gstatic.com/" crossorigin>
        <link rel="preconnect" href="https://khms0.googleapis.com" crossorigin>
        <link rel="preconnect" href="https://ia.iiitd.ac.in" crossorigin>
        <link rel="dns-prefetch" href="https://fonts.gstatic.com/">
        <link rel="dns-prefetch" href="https://maps.gstatic.com/">
        <link rel="dns-prefetch" href="https://khms0.googleapis.com">
        <link rel="dns-prefetch" href="https://maps.googleapis.com">
        <link rel="dns-prefetch" href="https://ia.iiitd.ac.in">
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" rel="preload" as="style" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css" rel="stylesheet" rel="preload" as="style" integrity="sha384-7/Tl0k65OTvDSvtuq7aPR7aa0aCz7ZKqHsbMRLxhzueldW+9MZpCe9LB1c5UBuNS" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="/static/css/main.css">
    </head>
    <body>
        <style>
    button[aria-expanded="true"]{
        border-bottom: solid 4px #40ADA7;
    }
    .dropdown-submenu {
       position: relative;
    }
    .dropdown-submenu>.dropdown-menu {
        top: 0;
        left: 100%;
    }
    @supports ((-webkit-backdrop-filter: none) or (backdrop-filter: none)) {
        .dropdown-menu {
            -webkit-backdrop-filter: blur(10px); backdrop-filter: blur(10px); background-color: rgba(0, 0, 0, 0.5); 
        }
    }
    @supports not ((-webkit-backdrop-filter: none) or (backdrop-filter: none)) {
        .dropdown-menu {
            background-color: rgb(0, 0, 0, 0.8);
            filter: blur(0.5px);
        }
    }
</style>
<div style="height: 70px;"></div>
<nav class="fixed-top" style="z-index: 998; background: black; height: 110px;">
    <div class="py-1 container-fluid bg-white">
        <div class="container">
            <div class="row px-md-4 d-md-flex align-items-center justify-content-between" style="height: 30px;">
                <ul class="list-unstyled">
                    <li class="d-flex align-items-center justify-content-center">
                        <a aria-label="contact" href="/contact" class="no-transform-imp">
                            <img alt="contact icon" class="ml-2 ml-md-1" src="/static/img/Icon%20metro-contacts-mail.svg">
                        </a>
                        <a aria-label="contact" href="/contact" class="text-green no-transform-imp">
                            &nbsp;&nbsp;Contact Us
                        </a>
                    </li>
                </ul>
                <ul class="list-unstyled" style="width: 150px;">
                    <li class="d-flex align-items-center justify-content-between">
                        <a aria-label="twitter hcd" rel='noopener noreferrer' href="https://twitter.com/hcd_IIITD" class="text-decoration-none" target="_blank">
                            <svg viewBox="0 0 24 24" style="height: 25px; width: 25px;"><g><path style="fill: #1da1f2;" d="M23.643 4.937c-.835.37-1.732.62-2.675.733.962-.576 1.7-1.49 2.048-2.578-.9.534-1.897.922-2.958 1.13-.85-.904-2.06-1.47-3.4-1.47-2.572 0-4.658 2.086-4.658 4.66 0 .364.042.718.12 1.06-3.873-.195-7.304-2.05-9.602-4.868-.4.69-.63 1.49-.63 2.342 0 1.616.823 3.043 2.072 3.878-.764-.025-1.482-.234-2.11-.583v.06c0 2.257 1.605 4.14 3.737 4.568-.392.106-.803.162-1.227.162-.3 0-.593-.028-.877-.082.593 1.85 2.313 3.198 4.352 3.234-1.595 1.25-3.604 1.995-5.786 1.995-.376 0-.747-.022-1.112-.065 2.062 1.323 4.51 2.093 7.14 2.093 8.57 0 13.255-7.098 13.255-13.254 0-.2-.005-.402-.014-.602.91-.658 1.7-1.477 2.323-2.41z"></path></g></svg>
                        </a>
                        <a aria-label="facebook hcd" rel='noopener noreferrer' href="https://www.facebook.com/hcdzine" class="text-decoration-none mb-1" target="_blank">                    
                            <svg viewBox="0 0 36 36" style="height: 22px; width: 22px;" fill="url(#jsc_c_2)"><defs><linearGradient x1="50%" x2="50%" y1="97.0782153%" y2="0%" id="jsc_c_2"><stop offset="0%" stop-color="#0062E0"></stop><stop offset="100%" stop-color="#19AFFF"></stop></linearGradient></defs><path d="M15 35.8C6.5 34.3 0 26.9 0 18 0 8.1 8.1 0 18 0s18 8.1 18 18c0 8.9-6.5 16.3-15 17.8l-1-.8h-4l-1 .8z"></path><path fill='#fff' class="p361ku9c" d="M25 23l.8-5H21v-3.5c0-1.4.5-2.5 2.7-2.5H26V7.4c-1.3-.2-2.7-.4-4-.4-4.1 0-7 2.5-7 7v4h-4.5v5H15v12.7c1 .2 2 .3 3 .3s2-.1 3-.3V23h4z"></path></svg>                                 
                        </a>
                        <a aria-label="linkedin hcd" rel='noopener noreferrer' href="https://www.linkedin.com/in/human-centered-design-hcd-43b3a7180/" class="text-decoration-none" target="_blank">
                            <svg xmlns="http://www.w3.org/2000/svg" width="23" height="23" data-supported-dps="34x34" focusable="false">
                                <g transform="scale(0.45)" fill="none" fill-rule="evenodd">
                                  <rect class="bug-text-color" fill="#FFF" x="1" y="1" width="46" height="46" rx="4"></rect>
                                  <path d="M0 4.01A4.01 4.01 0 014.01 0h39.98A4.01 4.01 0 0148 4.01v39.98A4.01 4.01 0 0143.99 48H4.01A4.01 4.01 0 010 43.99V4.01zM19 18.3h6.5v3.266C26.437 19.688 28.838 18 32.445 18 39.359 18 41 21.738 41 28.597V41.3h-7V30.159c0-3.906-.937-6.109-3.32-6.109-3.305 0-4.68 2.375-4.68 6.109V41.3h-7v-23zM7 41h7V18H7v23zm8-30.5a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0z" class="background" fill="#0077B5"></path>
                                </g>
                            </svg>
                        </a>
                        <a aria-label="instagram hcd" rel='noopener noreferrer' href="https://www.instagram.com/hcd.iiitd" class="text-decoration-none" target="_blank">                    
                            <img src="../static/images/insta_svg.svg" style="height: 23px; width: 23px;">
                        </a>
                    </li>
                </ul>
            </div>
        </div> 
    </div>
    
    <div class="container mt-2">
        <div class="d-flex align-items-center justify-content-between">
            <a href="/"><img src="/static/images/logoleft.svg" style="height: 50px;" alt="Indraprastha Institute Of Information Technology, Delhi"><img class="ml-2" src="/static/images/HCDLOGO.png" style="height:60px;" alt="Human Centered Design"></a>
            <div class="d-flex">
                <div class="dropdown d-none d-lg-block">
                    <button class="btn text-pointer px-3 text-white font-12" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        Academics
                    </button>
                    <div class="border-0 btn btn-lg dropdown-menu" aria-labelledby="dropdownMenuButton">
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/btech-courses/">
                            B.Tech. in CSD
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/phd/">
                            Ph.D.
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/thesis-defense/">
                            Thesis Defense
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/student-policy/">
                            Student Conduct Policy
                        </a>
                    </div>
                </div>
                <div class="dropdown d-none d-lg-block">
                    <button class="btn text-pointer px-3 text-white font-12" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        Research
                    </button>
                    <div class="border-0 btn btn-lg dropdown-menu" aria-labelledby="dropdownMenuButton">
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/research-areas/">
                            Research Areas
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/research-labs/">
                            Labs
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/research-publications/">
                            Research Publications
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/research-projects/">
                            Research Projects
                        </a>
                    </div>
                </div>
                <div class="dropdown d-none d-lg-block">
                    <button class="btn text-pointer px-3 text-white font-12" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        People
                    </button>
                    <div class="border-0 btn btn-lg dropdown-menu" aria-labelledby="dropdownMenuButton">
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/students/">
                            B.Tech. Students
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/phd-scholars/">
                            Ph.D. Scholars
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/faculty/">
                            Faculty
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="/staff/">
                            Staff
                        </a>
                    </div>
                </div>
                <div class="dropdown d-none d-lg-block">
                    <button class="btn text-pointer px-3 text-white font-12" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        Events
                    </button>
                    <div class="border-0 btn btn-lg dropdown-menu" aria-labelledby="dropdownMenuButton">
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="">
                            Seminars
                        </a>
                        <a class="h5 dropdown-item text-white" onmouseover="this.classList.remove('text-white');this.classList.add('text-dark');" onmouseout="this.classList.add('text-white');this.classList.remove('text-dark');" href="">
                            Workshops
                        </a>
                    </div>
                </div>
                <div class="dropdown d-none d-lg-block">
                    <button class="btn text-pointer px-3 text-white font-12" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        Courses
                    </button>
                </div>
            </div>
            <!-- <a class="d-lg-block d-none mb-2" id="search" onmouseover="this.style.cursor='pointer'">
                <div class="d-flex rounded input" style="position: relative;">
                    <form aria-label="Desktop Search" style="margin:0px; margin-top:4px; width: 300px;" action="https://search.freefind.com/find.html" method="get" accept-charset="utf-8" target="_self">
                        <input type="hidden" name="si" value="10569528">
                        <input type="hidden" name="pid" value="r">
                        <input type="hidden" name="n" value="0">
                        <input type="hidden" name="_charset_" value="">
                        <input type="hidden" name="bcd" value="&#247;">
                        <input aria-label="Search Desktop" style="border-radius: 5px !important;" class="animated fadeIn faster shadow btn btn-lg btn-white bg-white no-transform text-gray w-100" type="text" name="query" placeholder="Search">
                        <input class="d-none" type="submit" value="search" id="tosearch_desktop">
                    </form>
                    <img src="/static/images/search.svg" height="30px" style="margin-top: 0.8em; position: absolute; left: 10px;" alt="Search Bar">
                </div>
            </a> -->
            <a onclick="togglemenu('null')" class="d-lg-none" onmouseover="this.style.cursor='pointer'">
                <img id="nav_menu" src="/static/images/menu.svg" height="35px" alt="Navigation Menu">
            </a>
        </div>
    </div>

    <div id="menu" class="animated fadeIn faster container-fluid d-lg-none px-5 py-2" style="position: absolute; top: 110px; overflow-y:scroll; background: #E1E1E1; height: 310px; border-radius: 10px;">
        <div class="row d-md-flex d-lg-none justify-content-center">
            <div class="col-12 mt-3 text-center rounded">
                <div class="row py-2">
                    <div class="d-flex rounded input w-100" style="position: relative;">
                        <form style="margin:0px; margin-top:4px; width: 100%;" action="https://search.freefind.com/find.html" method="get" accept-charset="utf-8" target="_self">
                            <input type="hidden" name="si" value="10569528">
                            <input type="hidden" name="pid" value="r">
                            <input type="hidden" name="n" value="0">
                            <input type="hidden" name="_charset_" value="">
                            <input type="hidden" name="bcd" value="&#247;">
                            <input class="animated fadeIn faster btn btn-lg btn-white bg-white no-transform text-gray w-100" style="border-radius: 5px !important;" type="text" name="query" placeholder="Search">
                            <input class="d-none" type="submit" value="search" id="tosearch_mobile">
                        </form>
                        <img src="/static/images/search.svg" height="30px" style="margin-top: 0.8em; position: absolute; left: 10px;" alt="Search Bar">
                    </div>
                </div>
            </div>
            <div class="col-12 d-lg-none" style="background-color: #E1E1E1;">
                <ul class="list-unstyled pb-2 text-center">
                    <li class="mt-3"> 
                        <button id="Academics_Inner" onclick="handleinnermenu('Academics', this.id);" class="menu-option-1 btn btn-block btn-white btn-lg text-pointer rounded">
                            Academics&nbsp;&nbsp;
                            <img id="academics_svg" style="padding-bottom: 0.2em;" src="/static/images/arrow.svg">
                        </button>
                    </li>
                    <li id="academics_innermenu" class="d-none mt-2 animated fadeIn">
                        <ul class="list-unstyled p-2 rounded text-center" style="background: #F2F2F2;">
                            <li class="mt-2">
                                <p onclick="location.href='../btech-courses/'">B.Tech. in CSD</p>
                            </li>
                            <li class="">
                                <p onclick="location.href='../phd/'">Ph.D.</p>
                            </li>
                            <li class="">
                                <p onclick="location.href='../thesis-defense/'">Thesis Defense</p>
                            </li>
                            <li class="">
                                <p onclick="location.href='../student-policy/'">Student Conduct Policy</p>
                            </li>
                        </ul>
                    </li>
                    <li class="mt-3"> 
                        <button id="Research_Inner" onclick="handleinnermenu('Research', this.id);" class="menu-option-2 btn btn-block btn-white btn-lg text-pointer rounded">
                            Research&nbsp;&nbsp;
                            <img id="research_svg" style="padding-bottom: 0.2em;" src="/static/images/arrow.svg">
                        </button>
                    </li>
                    <li id="research_innermenu" class="d-none mt-2 animated fadeIn">
                        <ul class="list-unstyled p-2 rounded text-center" style="background: #F2F2F2;">
                            <li class="mt-2">
                                <p onclick="location.href='../research-areas/'">Research Areas</p>
                            </li>
                            <li class="">
                                <p onclick="location.href='../research-labs/'">Research Labs</p>
                            </li>
                            <li class="">
                                <p onclick="location.href='../research-publications/'">Research Publications</p>
                            </li>
                            <li class="">
                                <p onclick="location.href='../research-projects/'">Research Projects</p>
                            </li>
                        </ul>
                    </li>
                    <li class="mt-3"> 
                        <button id="People_Inner" onclick="handleinnermenu('People', this.id);" class="menu-option-3 btn btn-block btn-white btn-lg text-pointer rounded">
                            People&nbsp;&nbsp;<img id="people_svg" style="padding-bottom: 0.2em;" src="/static/images/arrow.svg">
                        </button>
                    </li>
                    <li id="people_innermenu" class="d-none mt-2 animated fadeIn">
                        <ul class="list-unstyled py-2 rounded text-center" style="background: #F2F2F2;">
                            <li class="">
                                <p onclick="location.href='../students/'">B.Tech. Students</p>
                            </li>
                            <li class="mt-2">
                                <p onclick="location.href='../phd-scholars/'">Ph.D. Scholars</p>
                            </li>
                            <li class="mt-2">
                                <p onclick="location.href='../faculty/'">Faculty</p>
                            </li>
                            <li class="">
                                <p onclick="location.href='../staff/'">Staff</p>
                            </li>
                        </ul>
                    </li>
                </ul>      
            </div>
        </div>
    </div>
    
    <div id="menu_2" class="animated fadeIn faster container-fluid d-none d-lg-none px-5 py-2" style="position: absolute; top: 70px; overflow-y:scroll; background: transparent; height: 250px; border-radius: 10px;">
        <div class="row d-lg-flex align-items-center justify-content-center">
            <div class="col-8 border" style="background-color: #E1E1E1; border-radius: 15px; border-color: rgb(161, 161, 161) !important;">
                <ul id="research_2" class="list-unstyled d-flex justify-content-between mt-4 text-center">
                    <li class="font-12 text-pointer">
                        <p onclick="location.href='../research-areas/'">Research Areas</p>
                    </li>
                    <li class="font-12 text-pointer">
                        <p onclick="location.href='../research-labs'">Research Labs</p>
                    </li>
                    <li class="font-12 text-pointer">
                        <p onclick="location.href='../research-publications/'">Research Publications</p>
                    </li>
                </ul>
                <ul id="people_2" class="list-unstyled d-flex justify-content-center mt-4 text-center">
                    <li class="font-12 text-pointer">
                        <p onclick="location.href='../faculty/'">Faculty</p>
                    </li>
                    <li class="font-12 text-pointer ml-5">
                        <p onclick="location.href='../students/'">Students</p>
                    </li>
                </ul>
                <ul id="academics_2" class="list-unstyled d-flex justify-content-between mt-4 text-center">
                    <li class="font-12 text-pointer">
                        <p onclick="location.href='../btech-courses/'">B.Tech. in CSD</p>
                    </li>
                    <li class="font-12 text-pointer">
                        <p onclick="location.href='../phd/'">Ph.D.</p>
                    </li>
                    <li class="font-12 text-pointer">
                        <p onclick="location.href='../student-policy/'">Student Conduct Policy</p>
                    </li>
                </ul>  
                <li class="list-unstyled mt-5 pb-2">
                    <ul class="d-flex text-center align-items-end flex-column h-100 mt-2 mb-0" style="list-style: none;">
                        <li class="w-100">Follow Us</li>
                        <li class="w-100 mt-1">
                            <a rel='noopener noreferrer' href="https://twitter.com/hcd_IIITD" class="text-decoration-none" target="_blank">
                                <img src="/static/images/twitter_svg.svg" height="40">
                            </a>
                            <a rel='noopener noreferrer' href="https://www.facebook.com/hcdzine" class="text-decoration-none" target="_blank">
                                <img src="/static/images/facebook_hcd.svg" height="40">
                            </a>
                            <a rel='noopener noreferrer' href="https://www.linkedin.com/in/human-centered-design-hcd-43b3a7180/" class="text-decoration-none" target="_blank">
                                <img src="/static/images/linkedin_hcd.svg" height="40">
                            </a>
                        </li>
                    </ul>
                </li>
            </div>
        </div>
    </div>
</nav>

<script>
    function openbiggermenu(lol){
        lol= lol.toString().toLowerCase();
        if($('#menu_2').hasClass('d-lg-block')){
            $('#menu_2').addClass('d-lg-none');
            if(!($('#'+lol+'_2').hasClass('d-lg-none'))){
                $('#menu_2').removeClass('d-lg-block');
            }
            else{
                let k= ['academics_2', 'research_2', 'people_2'];
                for(let i in k){
                    $('#'+k[i]).addClass('d-lg-none');
                    $('#'+k[i]).removeClass('d-lg-block');
                }
                $('#menu_2').removeClass('d-lg-none');
                $('#menu_2').addClass('d-lg-block');
                $('#'+lol+'_2').removeClass('d-lg-none');
            }
        }
        else{
            let k= ['academics_2', 'research_2', 'people_2'];
            for(let i in k){
                $('#'+k[i]).addClass('d-lg-none');
                $('#'+k[i]).removeClass('d-lg-block');
            }
            $('#menu_2').removeClass('d-lg-none');
            $('#menu_2').addClass('d-lg-block');
            $('#'+lol+'_2').removeClass('d-lg-none');
        }
    }
</script>
        
<style>
    .fix-publications{
        max-height: 700px; 
        overflow: scroll;
    } 
    .listing {
        text-decoration: none;
    }
    .listing:hover {
        text-decoration-color: #17a2b8 !important; 
        border-bottom: 1px solid #17a2b8 !important;
    }
    .scrolltoptop{
        position: fixed;
        right: 0;
        bottom: 0;
        padding-right: 10px;
    }
    @media screen and (max-width: 768px){
        .fix-publications{
            max-height: unset !important;
            overflow: unset !important;
        }   
    }
</style>
<ul class="scrolltoptop d-md-none mt-3 w-100" style="z-index: 10;">
    <span class="d-flex justify-content-between">
        <span></span>
        <span class="bg-light p-2 rounded text-gray" onclick="$('html, body').animate({ scrollTop: 0 }, 'slow');">
            Scroll to top
            <svg style="width: 10px;" aria-hidden="true" focusable="false" data-prefix="fal" data-icon="long-arrow-up" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><path fill="currentColor" d="M3.515 168.97l7.07 7.071c4.686 4.686 12.284 4.686 16.971 0L111 92.113V468c0 6.627 5.373 12 12 12h10c6.627 0 12-5.373 12-12V92.113l83.444 83.928c4.686 4.686 12.284 4.686 16.971 0l7.07-7.071c4.686-4.686 4.686-12.284 0-16.97l-116-116.485c-4.686-4.686-12.284-4.686-16.971 0L3.515 152c-4.687 4.686-4.687 12.284 0 16.97z" class=""></path></svg>
        </span>
    </span>
</ul>
<div class="container py-5">
    <div class="d-flex flex-row">
        <h4 class="ml-2 mt-4 animated flipInX font-weight-bold">Research Publications</h4>
    </div>
    <div class="col-12">
        <div class="row">
            <!-- <div class="mt-3 col-12 col-md-3 order-0">
                 <div class="w-100">
                    <h6 class="text-gray">Filters</h6>
                </div> 
                <div class="btn-group-horizontal" role="group" aria-label="Vertical button group">
                    <button onclick="$('#myInput').val('2020'); myFunction(); $('html, body').animate({ scrollTop: $('.fix-publications').offset().top - ($(window).width()<768 ? 130 : 1000) }, 'slow');" type="button" class="mt-3 btn btn-sm btn-outline-green">2020</button>
                    <button onclick="$('#myInput').val('2019'); myFunction(); $('html, body').animate({ scrollTop: $('.fix-publications').offset().top - ($(window).width()<768 ? 130 : 1000) }, 'slow');" type="button" class="mt-3 btn btn-sm btn-outline-green">2019</button>
                    <button onclick="$('#myInput').val('2018'); myFunction(); $('html, body').animate({ scrollTop: $('.fix-publications').offset().top - ($(window).width()<768 ? 130 : 1000) }, 'slow');" type="button" class="mt-3 btn btn-sm btn-outline-green">2018</button>
                    <button onclick="$('#myInput').val('2017'); myFunction(); $('html, body').animate({ scrollTop: $('.fix-publications').offset().top - ($(window).width()<768 ? 130 : 1000) }, 'slow');" type="button" class="mt-3 btn btn-sm btn-outline-green">2017</button>
                </div> 
            </div> -->
            <div class="mt-4 mt-md-3 col-12 col-md-9 order-1">
                <div class="row fix-publications">
                    <!-- <input class="w-100 btn form-control btn-block bg-light text-dark border rounded text-left" type="text" id="myInput" onkeyup="myFunction()" placeholder="Search for publications..." title="Type in a name"> -->
                    <ul id="myUL" class="list-unstyled">
                        
                            
                                <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2019"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Dr. Aman Parnami</h5>
                                                <!-- <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2019</span> -->
                                            </div>
                                            <ol class="text-gray gara-proreg" style="font-size: 1.2em;">
                                                <li>
                                                    <p><b>Title:</b> Akash Chaudhary, Manshul Belani, Naman Maheshwari, and Aman Parnami.
                                                        2021. Verbose : Designing a Context-based Educational System for Improving
                                                        Communicative Expressions. In Proceedings of the 23rd International Conference on
                                                        Mobile Human-Computer Interaction (MobileHCI '21). Association for Computing
                                                        Machinery, New York, NY, USA, Article 41, 1–13.</p>
                                                        <p></b>DOI:</b> https://doi.org/10.1145/3447526.3472057</p>
                                                        <p><b>Abstract:</b> ESL (English as a second language) speakers tend to follow the tone
                                                            structure of their first language, making their speech difficult to understand for native
                                                            speakers, thereby limiting their opportunities for education and employment. To
                                                            address this problem, we build an interactive smartphone-based educational mobile
                                                            application using the user-centered design process. This application teaches English
                                                            intonations based on globally consistent pitch patterns through conversations with a
                                                            trained chat assistant, which inculcates expert linguists’ teaching principles. After
                                                            co-designing the application’s parameters with primary stakeholders and expert
                                                            visual designers, we assess its effectiveness by measuring the pre and
                                                            post-performance of the users after the system usage, using various quantitative
                                                            measures, like intonation scores, SEQ, and SUS. Feedback from users suggests that ESL
                                                            speakers find significant improvement in the perception of their vocal expressions,
                                                            thereby highlighting the necessity of such a system in improving the quality of
                                                            conversations that people have in general.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Dhruv Verma, Sejal Bhalla, Dhruv Sahnan, Jainendra Shukla, and Aman Parnami.
                                                        2021. ExpressEar: Sensing Fine-Grained Facial Expressions with Earables. <i>Proc.
                                                        ACM Interact. Mob. Wearable Ubiquitous Technol.</i> 5, 3, Article 129 (Sept 2021), 28
                                                        pages.</p>
                                                        <p><b>DOI:</b> https://doi.org/10.1145/3478085</p>
                                                        <p><b>Abstract:</b> Continuous and unobtrusive monitoring of facial expressions holds
                                                            tremendous potential to enable compelling applications in a multitude of domains
                                                            ranging from healthcare and education to interactive systems. Traditional,
                                                            vision-based facial expression recognition (FER) methods, however, are vulnerable to
                                                            external factors like occlusion and lighting, while also raising privacy concerns
                                                            coupled with the impractical requirement of positioning the camera in front of the
                                                            user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that
                                                            repurposes commercial earables augmented with inertial sensors to capture
                                                            fine-grained facial muscle movements. Following the Facial Action Coding System (FACS), which encodes every possible expression in terms of constituent facial
                                                            movements called Action Units (AUs), ExpressEar identifies facial expressions at the
                                                            atomic level. We conducted a user study (N=12) to evaluate the performance of our
                                                            approach and found that ExpressEar can detect and distinguish between 32 Facial AUs
                                                            (including 2 variants of asymmetric AUs), with an average accuracy of 89.9% for any
                                                            given user. We further quantify the performance across different mobile scenarios in
                                                            presence of additional face-related activities. Our results demonstrate ExpressEar's
                                                            applicability in the real world and open up research opportunities to advance its
                                                            practical adoption.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Arpit Bhatia, Dhruv Kundu, Suyash Agarwal, Varnika Kairon, and Aman Parnami.
                                                        2021. Soma-noti: Delivering Notifications Through Under-clothing Wearables.
                                                        <i>Proceedings of the 2021 CHI Conference on Human Factors in Computing
                                                        Systems</i>. Association for Computing Machinery, New York, NY, USA, Article 221,
                                                        1–8.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3411764.3445123</p>
                                                    <p><b>Abstract:</b> Different form factors of wearable technology provide unique opportunities
                                                        for output based on how they are connected to the human body. In this work, we
                                                        investigate the idea of delivering notifications through devices worn on the underside
                                                        of a user’s clothing. A wearable worn in such a manner is in direct contact with the
                                                        user’s skin. We leverage this proximity to test the performance of 10 on-skin
                                                        sensations (Press, Poke, Pinch, Heat, Cool, Blow, Suck, Vibrate, Moisture and Brush) as
                                                        methods of notification delivery. We developed prototypes for each stimulus and
                                                        conducted a user study to evaluate them across 6 locations commonly covered by
                                                        upper body clothing. Results indicate significant differences in reaction time, error
                                                        rates and comfort which may influence the design of future under-clothing wearables.</p>
                                                </li>
                                            </ol>  </div>
                                    </a>
                                </li>

                                <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2019"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Dr. Grace Eden</h5>
                                                <!-- <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2019</span> -->
                                            </div>
                                            <ol class="text-gray gara-proreg" style="font-size: 1.2em;">
                                                <li>
                                                    <p><b>Title:</b> Singh A., Eden G. (2021) Hanging Out Online: Social Life During the Pandemic.
                                                        In: Ardito C. et al. (eds) Human-Computer Interaction – INTERACT 2021. INTERACT
                                                        2021. Lecture Notes in Computer Science, vol 12933. Springer, Cham</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1007/978-3-030-85616-8_2</p>
                                                    <p><b>Abstract:</b> In March 2020, the government of India ordered a nationwide lockdown to
                                                        prevent the spread of Covid-19. This led to the shutdown of educational institutes
                                                        throughout the country, restricting all activities to online mediums. The shift has
                                                        affected how students engage with each other, where rather than in-person
                                                        interaction, they meet through a variety of online tools. In this paper, we discuss how
                                                        the normal everyday routine of ‘hanging out’ with friends has been transformed
                                                        during a prolonged lockdown of over ten months and counting. We investigate the
                                                        opportunities and challenges students encounter when socializing online through
                                                        various online modes including video calls, communal movie watching and social
                                                        media. We discuss how social interaction; in particular, hanging out with friends has
                                                        been transformed through these technologies and its implications for facilitating
                                                        spontaneous interaction, negotiating intimacy, mutual understanding, and
                                                        accessibility to different social groups. Finally, we conclude with a discussion of how
                                                        these factors impact the transition from in-person to online modes of casual social
                                                        interaction.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Sumita Sharma, Netta Iivari, Marianne Kinnula, Grace Eden, Alipta Ballav, Rocio
                                                        Fatas, Ritwik Kar, Deepak Ranjan Padhi, Vahid Sadeghie, Pratiti Sarkar, Riya Sinha,
                                                        Rucha Tulaskar, and Nikita Valluri. 2021. From Mild to Wild: Reimagining Friendships
                                                        and Romance in the Time of Pandemic Using Design Fiction. In <i>Designing
                                                        Interactive Systems Conference 2021</i> (<i>DIS '21</i>). Association for
                                                        Computing Machinery, New York, NY, USA, 64–77.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3461778.3462110</p>
                                                    <p><b>Abstract:</b> With the forced reboot of our lives due to the COVID-19 pandemic, our
                                                        interpersonal relationships are nowhere yet everywhere. However, opportunities for
                                                        initiating or maintaining friendships and romance in the physical world have
                                                        dwindled. Within the context of India where multiple realities exist, the question
                                                        arises – what is the future of these relationships? In this paper, we present the
                                                        outcomes of a workshop looking at the future of relationships using design fiction.
                                                        Participants worked in small teams to create scenarios that critically consider the
                                                        future of love, friendships, and romance within the Indian context. Through the lenses
                                                        of criticality, empowerment, and value creation, we examine the design scenarios and
                                                        the design process including criticality of the designs, empowering experiences of the
                                                        participants, and the perceived value gained from participating in such a workshop. Our findings indicate the potential of design fiction to allow participants to step out of
                                                        their comfort zone into a critical stance in discussing love and intimacy. Based upon
                                                        our findings, we discuss implications for design research, practice, and education.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Marianne Kinnula, Netta Iivari, Sumita Sharma, Grace Eden, Markku Turunen,
                                                        Krishnashree Achuthan, Prema Nedungadi, Tero Avellan, Biju Thankachan, and Rucha
                                                        Tulaskar. 2021. Researchers’ Toolbox for the Future: Understanding and Designing
                                                        Accessible and Inclusive Artificial Intelligence (AIAI). In <i>Academic Mindtrek
                                                        2021</i> (<i>Mindtrek 2021</i>). Association for Computing Machinery, New York,
                                                        NY, USA, 1–4.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3464327.3464965</p>
                                                    <p><b>Abstract:</b> As Artificial Intelligence (AI) is integrated into all things technical, there is a
                                                        valid concern over its lack of diversity, inclusiveness, and accessibility. Further,
                                                        questions such as what it means for AI to be accessible and inclusive, why is inclusive
                                                        AI required, and how can it be achieved, is an emerging area of research. In this
                                                        two-part workshop, we will explore the nuanced challenges towards Accessible and
                                                        Inclusive AI together with participants with diverse backgrounds. First, we will
                                                        collaboratively define Accessible and Inclusive AI (AIAI), building on the diverse
                                                        experiences of the participants and moderators. The goal is to contribute to the
                                                        formulation of a shared vision for Accessibility and AI as well as identify the
                                                        challenges and opportunities towards realizing this vision. Working in small teams,
                                                        participants will collaboratively conceptually design a future scenario for AIAI or
                                                        critically analyse an example solution. The aim is for teams to tackle tough questions
                                                        related to what it means for AI to be accessible and inclusive, while addressing
                                                        algorithmic biases and limitations of AI, in addition to opportunities for overcoming
                                                        them in the future. Finally, teams will present their conceptual designs and scenarios
                                                        in the larger group. Overall, the workshop will ignite innovative, and even provocative,
                                                        ideas and future scenarios, building towards an inclusive and accessible AI.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Arpit Bhatia, Aneesha Lakra, Rakshita Anand, and Grace Eden. 2021. An
                                                        Analysis of Ludo Board Game Play on Smartphones. In <i>Extended Abstracts of the
                                                        2021 CHI Conference on Human Factors in Computing Systems</i> (<i>CHI EA
                                                        '21</i>). Association for Computing Machinery, New York, NY, USA, Article 396, 1–6.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3411763.3451728</p>
                                                    <p><b>Abstract:</b> From sports to party games, almost every kind of game has been adapted
                                                        into a digital video game format. While previous research has studied player
                                                        motivations and experiences for certain categories of digital games, there has yet to
                                                        be such a study on digital board games, especially in the modern context of
                                                        smartphone apps. To address this, we conduct a case study of a popular board game,
                                                        Ludo, to understand players’ opinions of its digital adaptation. For this, we study the
                                                        functionality and user reviews of nine popular Ludo apps, to assess player opinions of
                                                        how traditional gameplay has been re-imagined. Based upon our analysis, we conclude with recommendations for improving Ludo apps and other apps, based on
                                                        random chance board games.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Noura Howell, Britta F. Schulte, Amy Twigger Holroyd, Rocío Fatás Arana,
                                                        Sumita Sharma, and Grace Eden. 2021. Calling for a Plurality of Perspectives on
                                                        Design Futuring: An Un-Manifesto. In <i>Extended Abstracts of the 2021 CHI
                                                        Conference on Human Factors in Computing Systems</i> (<i>CHI EA '21</i>).
                                                        Association for Computing Machinery, New York, NY, USA, Article 31, 1–10.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3411763.3450364</p>
                                                    <p><b>Abstract:</b> The Futures Cone, a prominent model in design futuring, is useful for
                                                        promoting discussions about possible, plausible, probable, and preferable futures. Yet
                                                        this model has limitations, such as representing diverse human experiences as a
                                                        singular point of “the present” and implicitly embedding notions of linear progress.
                                                        Responding to this, we argue that a plurality of perspectives is needed to engage
                                                        imaginations that depict a diverse unfolding of potential futures. Through reflecting
                                                        on our own cultural and professional backgrounds, we offer five perspectives for
                                                        design futuring as a contribution to this plurality: Parallel Presents, “I Am Time”,
                                                        Epithelial Metaphors, the Uncertainties Cone, and Meet (with) “Speculation”. These
                                                        perspectives open alternative approaches to design futuring, move outside prevalent
                                                        notions of technological progress, and foreground interdependent, relational
                                                        agencies.</p>
                                                </li> 
                                            </ol>                                       
                                             </div>
                                    </a>
                                </li>
                            
                                <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2019"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Dr. Jainendra Shukla</h5>
                                                <!-- <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2019</span> -->
                                            </div>
                                            <ol class="text-gray gara-proreg" style="font-size: 1.2em;">
                                                <li>
                                                    <p><b>Title:</b> Dhruv Verma, Sejal Bhalla, Dhruv Sahnan, Jainendra Shukla, and Aman
                                                        Parnami. 2021. ExpressEar: Sensing Fine-Grained Facial Expressions with Earables.
                                                        <i>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</i> 5, 3, Article 129 (Sept
                                                        2021), 28 pages.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3478085</p>
                                                    <p><b>Abstract:</b> Continuous and unobtrusive monitoring of facial expressions holds
                                                        tremendous potential to enable compelling applications in a multitude of domains
                                                        ranging from healthcare and education to interactive systems. Traditional,
                                                        vision-based facial expression recognition (FER) methods, however, are vulnerable to
                                                        external factors like occlusion and lighting, while also raising privacy concerns
                                                        coupled with the impractical requirement of positioning the camera in front of the
                                                        user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that
                                                        repurposes commercial earables augmented with inertial sensors to capture
                                                        fine-grained facial muscle movements. Following the Facial Action Coding System
                                                        (FACS), which encodes every possible expression in terms of constituent facial
                                                        movements called Action Units (AUs), ExpressEar identifies facial expressions at the atomic level. We conducted a user study (N=12) to evaluate the performance of our
                                                        approach and found that ExpressEar can detect and distinguish between 32 Facial
                                                        AUs (including 2 variants of asymmetric AUs), with an average accuracy of 89.9% for
                                                        any given user. We further quantify the performance across different mobile
                                                        scenarios in presence of additional face-related activities. Our results demonstrate
                                                        ExpressEar's applicability in the real world and open up research opportunities to
                                                        advance its practical adoption.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> K. Rana, R. Madaan and J. Shukla, "Effect of Polite Triggers in Chatbot
                                                        Conversations on User Experience across Gender, Age, and Personality," 2021 30th
                                                        IEEE International Conference on Robot & Human Interactive Communication
                                                        (RO-MAN), 2021, pp. 813-819</p>
                                                    <p><b>DOI:</b> 10.1109/RO-MAN50785.2021.9515528.</p>
                                                    <p><b>Abstract:</b> Chatbots are one of the emerging intelligent systems which interact with
                                                        customers to solve different queries in a wide range of domain areas. During social
                                                        interaction, politeness plays a vital role in achieving effective communication.
                                                        Consequently, it becomes essential to understand how a chatbot’s politeness affects
                                                        user experience during the interaction. To understand it, we conducted a
                                                        between-subject user study with two chatbots where one of the chatbots employs
                                                        polite triggers, and the other one replies intending to answer the queries. To
                                                        introduce politeness in normal chatbot responses, we used the state-of-the-art tag
                                                        and generate approach. We first analyzed how different personality traits influence
                                                        the response of individual persons to polite triggers. In addition, we also investigated
                                                        the effects of polite triggers among different genders and age groups using a
                                                        cross-sectional analysis.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> B. Ashwini, V. Narayan, A. Bhatia and J. Shukla, "Responsiveness towards
                                                        robot-assisted interactions among pre-primary children of Indian ethnicity," 2021
                                                        30th IEEE International Conference on Robot & Human Interactive Communication
                                                        (RO-MAN), 2021, pp. 619-625.</p>
                                                    <p><b>DOI:</b> 10.1109/RO-MAN50785.2021.9515520.</p>
                                                    <p><b>Abstract:</b> Today’s world is undeniably technology-driven and children are the ones
                                                        who adopt technology with ease. This fact could be leveraged to design assistive
                                                        technologies for children using social robots as they are observed to be efficient
                                                        pedagogical agents. Robotic technology is evolving rapidly and robots are designed to
                                                        play social roles in education, health care and home assistance. However, there has
                                                        been limited research focusing on the use of robotic technologies for designing
                                                        interactions with children in the global south, owing to which the response behaviour
                                                        towards robot-assisted interventions are unknown. To address this gap, we
                                                        conducted a study to understand the response behaviour of Indian children of the age
                                                        3-6 years towards robot-assisted interventions during directive tasks. Our analysis
                                                        shows that the children could follow to robot’s instructions during the tasks and complete the tasks successfully. The exploratory outcomes also highlight the
                                                        acceptance and benefits of using robotic assistants as a facilitator in education,
                                                        cognitive therapies and healthcare.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Singhal A., Goyal M., Shukla J., Mutharaju R. (2021) Feature Fused Human
                                                        Activity Recognition Network (FFHAR-Net). In: Stephanidis C., Antona M., Ntoa S.
                                                        (eds) HCI International 2021 - Posters. HCII 2021. Communications in Computer and
                                                        Information Science, vol 1420. Springer, Cham.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1007/978-3-030-78642-7_72</p>
                                                    <p><b>Abstract:</b> With the advances in smart home technology and Internet of Things (IoT),
                                                        there has been keen research interest in human activity recognition to allow service
                                                        systems to understand human intentions. Recognizing human objectives by these
                                                        systems without user intervention, results in better service, which is crucial to
                                                        improve the user experience. Existing research approaches have focused primarily on
                                                        probabilistic methods like Bayesian networks (for instance, the CRAFFT algorithm).
                                                        Though quite versatile, these probabilistic models may be unable to successfully
                                                        capture the possibly complex relationships between the input variables. To the best
                                                        of our knowledge, a statistical study of features in a human activity recognition task,
                                                        their relationships, etc., has not yet been attempted. To this end, we study the domain
                                                        of human activity recognition to improve the state-of-the-art and present a novel
                                                        neural network architecture for the task. It employs early fusion on different types of
                                                        minimalistic features such as time and location to make extremely accurate
                                                        predictions with a maximum micro F1-score of 0.98 on the Aruba CASAS dataset. We
                                                        also accompany the model with a comprehensive study of the features. Using feature
                                                        selection techniques like Leave-One-Out, we rank the features according to the
                                                        information they add to deep learning models and make further inferences using the
                                                        ranking obtained. Our empirical results show that the feature Previous Activity
                                                        Performed is the most useful of all, surprisingly even more than time (the basis of
                                                        activity scheduling in most societies). We use three Activities of Daily Living (ADL)
                                                        datasets in different settings to empirically demonstrate the utility of our
                                                        architecture. We share our findings along with the models and the source code.</p>
                                                </li>     
                                                <li>
                                                    <p><b>Title:</b> Vidit Jain, Maitree Leekha, Rajiv Ratn Shah, and Jainendra Shukla. 2021.
                                                        Exploring Semi-Supervised Learning for Predicting Listener Backchannels.
                                                        Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems.
                                                        Association for Computing Machinery, New York, NY, USA, Article 395, 1–12.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3411764.3445449</p>
                                                    <p><b>Abstract:</b> Developing human-like conversational agents is a prime area in HCI
                                                        research and subsumes many tasks. Predicting listener backchannels is one such
                                                        actively-researched task. While many studies have used different approaches for
                                                        backchannel prediction, they all have depended on manual annotations for a large
                                                        dataset. This is a bottleneck impacting the scalability of development. To this end, we propose using semi-supervised techniques to automate the process of identifying
                                                        backchannels, thereby easing the annotation process. To analyze our identification
                                                        module’s feasibility, we compared the backchannel prediction models trained on (a)
                                                        manually-annotated and (b) semi-supervised labels. Quantitative analysis revealed
                                                        that the proposed semi-supervised approach could attain 95% of the former’s
                                                        performance. Our user-study findings revealed that almost 60% of the participants
                                                        found the backchannel responses predicted by the proposed model more natural.
                                                        Finally, we also analyzed the impact of personality on the type of backchannel signals
                                                        and validated our findings in the user-study.</p>
                                                </li> 
                                            
                                            </ol>                                       
                                             </div>
                                    </a>
                                </li>
                            
                        
                            
                                <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2019"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Prof. Pushpendra Singh</h5>
                                                <!-- <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2019</span> -->
                                            </div>
                                            <ol class="text-gray gara-proreg" style="font-size: 1.2em;">
                                                <li>
                                                    <p><b>Title:</b> Deepika Yadav, Prerna Malik, Kirti Dabas, and Pushpendra Singh. 2021.
                                                        Illustrating the Gaps and Needs in the Training Support of Community Health
                                                        Workers in India. <i>Proceedings of the 2021 CHI Conference on Human Factors in
                                                        Computing Systems</i>. Association for Computing Machinery, New York, NY, USA,
                                                        Article 231, 1–16.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3411764.3445111</p>
                                                    <p><b>Abstract:</b> In India and other developing countries, Community Health Workers
                                                        (CHWs) provide the first line of care in delivering necessary maternal and child
                                                        health services. In this work, we assess the training and skill-building needs of CHWs,
                                                        through a mobile-based training intervention deployed for six months to 500 CHWs
                                                        for conducting 144 training sessions in rural India. We qualitatively probed 1178
                                                        questions, asked by CHWs, during training sessions, and conducted a content analysis
                                                        of the learning material provided to CHWs. Further, we interviewed 48 CHWs to
                                                        understand the rationale of information seeking and perceptions of training needs.
                                                        We present our understanding of the knowledge gaps of CHWs and how the current
                                                        learning material and training methods are ineffective in addressing it. Our study
                                                        presents design implications for HCI4D researchers for mobile learning platforms
                                                        targeted towards CHWs. We also provide policy-level suggestions to improve the
                                                        training of CHWs in India or a similar context.</p>
                                                </li>  
                                                <li>
                                                    <p><b>Title:</b> Ayushi Srivastava, Shivani Kapania, Anupriya Tuli, and Pushpendra Singh.
                                                        2021. Actionable UI Design Guidelines for Smartphone Applications Inclusive of
                                                        Low-Literate Users. <i>Proc. ACM Hum.-Comput. Interact.</i> 5, CSCW1, Article 136
                                                        (April 2021), 30 pages.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3449210</p>
                                                    <p><b>Abstract:</b> With easy access to affordable internet-powered smartphones, developing
                                                        countries are adopting smartphone applications to provide enabling services to its
                                                        citizens, through eHealth, eGovernance, and digital payments. The challenge is to ensure equitable access to these services by everyone, including people with
                                                        semi-literacy or low-literacy who form a large part of the population in developing
                                                        countries. However, extensive HCI literature has identified literacy as one of the
                                                        barriers to designing user interfaces. In this work, we propose a framework of
                                                        actionable guidelines for designing smartphone UIs that would be usable by
                                                        low-literate users. We reviewed the last two decades of HCI literature engaging
                                                        people with low literacy, to synthesize our framework-designing SARAL. To evaluate
                                                        the framework, we conducted a preliminary study with a group of 20 practitioners
                                                        and researchers working in the field of UI/UX/HCI. We also analyzed six publicly
                                                        available industry reports on designing UIs for people with low-literacy. The
                                                        proposed guidelines intend to support researchers, practitioners, designers, and
                                                        implementers in the design and evaluation of UIs of smartphone applications for
                                                        people with low literacy. We present the evolutionary nature of the proposed
                                                        framework while highlighting the importance of adopting a translational approach
                                                        when building such frameworks.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Ayushi Srivastava, Shivani Kapania, Anupriya Tuli, and Pushpendra Singh. 2021.
                                                        Actionable Guidelines for Mobile Applications Inclusive of Low-Literate Users. 30
                                                        pages. InProceedings of the ACM on Human-Computer Interaction.CSCW’ 2021.
                                                        (CORE A, ranked number 2 venue by Google Scholar in HCI area with h-median 86).</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3449210</p>
                                                    <p><b>Abstract:</b> With easy access to affordable internet-powered smartphones, developing
                                                        countries are adopting smartphone applications to provide enabling services to its
                                                        citizens, through eHealth, eGovernance, and digital payments. The challenge is to
                                                        ensure equitable access to these services by everyone, including people with
                                                        semi-literacy or low-literacy who form a large part of the population in developing
                                                        countries. However, extensive HCI literature has identified literacy as one of the
                                                        barriers to designing user interfaces. In this work, we propose a framework of
                                                        actionable guidelines for designing smartphone UIs that would be usable by
                                                        low-literate users. We reviewed the last two decades of HCI literature engaging
                                                        people with low literacy, to synthesize our framework—designing SARAL. To
                                                        evaluate the framework, we conducted a preliminary study with a group of 20
                                                        practitioners and researchers working in the field of UI/UX/HCI. We also analyzed six
                                                        publicly available industry reports on designing UIs for people with low-literacy. The
                                                        proposed guidelines intend to support researchers, practitioners, designers, and
                                                        implementers in the design and evaluation of UIs of smartphone applications for
                                                        people with low literacy. We present the evolutionary nature of the proposed
                                                        framework while highlighting the importance of adopting a translational approach
                                                        when building such frameworks</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Deepika Yadav, Prerna Malik, Kirti Dabas, and Pushpendra Singh. 2021. Illustrating
                                                        the gaps and Needs in the Training Support of Community Health Workers in India.
                                                        30 pages.In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems.CHI’2021. (CORE A*, ranked number 1 venue by Google Scholar in HCI area
                                                        with h-median122 and also one of the top 10 Computer Science Conferences)</p>
                                                    <p><b>DOI:</b> http://repository.iiitd.edu.in/xmlui/handle/123456789/932</p>
                                                    <p><b>Abstract:</b> Community health workers (CHWs) in low- and middle-income countries
                                                        play a vital role in public healthcare. CHWs particularly assist in improving maternal
                                                        and child health conditions of the poor and vulnerable who often remain unaware of
                                                        the available services and face socio-cultural barriers in accessing the health
                                                        services. India, which is still undergoing a burden of high child mortality, implements
                                                        its CHW program as a flagship program with close to a million CHWs appointed
                                                        across its states. However, under-training significantly limits the ability of CHWs to
                                                        provide quality services. We address the training problem in India by designing and
                                                        deploying low-cost mobile training tools that can complement the existing
                                                        face-to-face training mechanisms. Our system adopts a hybrid architecture to use
                                                        Interactive Voice Response for facilitating online audio training sessions. Thus,
                                                        allowing CHWs to access training from anywhere through their feature phones, a key
                                                        need that has been well recognized by HCI4D research. We contribute on the
                                                        following aspects: (1) Testing the feasibility and efficacy of our training tool through
                                                        a controlled field experiment (2) Unpacking the training needs of CHWs by analyzing
                                                        a question and answer record of 1178 and mapping it back to the existing reference
                                                        material through a large-scale deployment on 500 CHWs, (3) Investigating the
                                                        potential for peer-to-peer learning models to address the challenge of experts
                                                        availability through a controlled field experiment, and (4) Finally, exploring the
                                                        potential for automated techniques in this domain by proposing a semi-automated
                                                        NLP approach for curating generated learning content and exposing CHWs and
                                                        women to Chabot-based education for the first time. By using a range of mixed
                                                        methods and field experiments, this dissertation expands the focus of HCI4D and
                                                        mHealth research on CHWs competence development in low-resource settings, an
                                                        area that has long been neglected.</p>
                                                </li> 

                                            </ol> 
                                            </div>
                                    </a>
                                </li>

                                <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2019"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Dr. Rajiv Ratn Shah</h5>
                                                <!-- <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2019</span> -->
                                            </div>
                                            <ol class="text-gray gara-proreg" style="font-size: 1.2em;">
                                                <li>
                                                    <p><b>Title:</b> Sharma, D., Kumar, B., Chand, S. et al. A Trend Analysis of Significant Topics
                                                        Over Time in Machine Learning Research. SN COMPUT. SCI. 2, 469 (2021).</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1007/s42979-021-00876-2</p>
                                                    <p><b>Abstract:</b> A vast number of research papers on numerous topics publish every year
                                                        in different conferences and journals. Thus, it is difficult for new researchers to
                                                        identify research problems and topics manually, which research community is
                                                        currently focusing on. Since such research problems and topics help researchers to
                                                        be updated with new topics in research, it is essential to know trends in research
                                                        based on topic significance over time. Therefore, in this paper, we propose a method
                                                        to identify the trends in machine learning research based on significant topics over
                                                        time automatically. Specifically, we apply a topic coherence model with latent
                                                        Dirichlet allocation (LDA) to evaluate the optimal number of topics and significant
                                                        topics for a dataset. The LDA model results in topic proportion over documents
                                                        where each topic has its probability (i.e., topic weight) related to each document.
                                                        Subsequently, the topic weights are processed to compute average topic weights per
                                                        year, trend analysis using rolling mean, topic prevalence per year, and topic
                                                        proportion per journal title. To evaluate our method, we prepare a new dataset
                                                        comprising of 21,906 scientific research articles from top six journals in the area of
                                                        machine learning published from 1988 to 2017. Extensive experimental results on
                                                        the dataset demonstrate that our technique is efficient, and can help upcoming
                                                        researchers to explore the research trends and topics in different research areas,
                                                        say machine learning.</p>
                                                </li>  
                                                <li>
                                                    <p><b>Title:</b> Parekh, S., Kumar, Y. S., Singh, S., Chen, C., Krishnamurthy, B., & Shah, R. R.
                                                        (2021). MINIMAL: Mining Models for Data Free Universal Adversarial Triggers.
                                                        arXiv preprint arXiv:2109.12406.</p>
                                                    <p><b>Link:</b> arXiv:2109.12406</p>
                                                    <p><b>Abstract:</b> A vast number of research papers on numerous topics publish every year
                                                        in different conferences and journals. Thus, it is difficult for new researchers to
                                                        identify research problems and topics manually, which research community is
                                                        currently focusing on. Since such research problems and topics help researchers to
                                                        be updated with new topics in research, it is essential to know trends in research
                                                        based on topic significance over time. Therefore, in this paper, we propose a method
                                                        to identify the trends in machine learning research based on significant topics over
                                                        time automatically. Specifically, we apply a topic coherence model with latent
                                                        Dirichlet allocation (LDA) to evaluate the optimal number of topics and significant
                                                        topics for a dataset. The LDA model results in topic proportion over documents
                                                        where each topic has its probability (i.e., topic weight) related to each document.
                                                        Subsequently, the topic weights are processed to compute average topic weights per year, trend analysis using rolling mean, topic prevalence per year, and topic
                                                        proportion per journal title. To evaluate our method, we prepare a new dataset
                                                        comprising of 21,906 scientific research articles from top six journals in the area of
                                                        machine learning published from 1988 to 2017. Extensive experimental results on
                                                        the dataset demonstrate that our technique is efficient, and can help upcoming
                                                        researchers to explore the research trends and topics in different research areas,
                                                        say machine learning.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Kabra, A., Bhatia, M., Kumar, Y., Li, J. J., Jin, D., & Shah, R. R. (2021). Calling Out
                                                        Bluff: Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring
                                                        Systems.</p>
                                                    <p><b>Link:</b> arXiv:2007.06796</p>
                                                    <p><b>Abstract:</b> Automatic scoring engines have been used for scoring approximately
                                                        fifteen million test takers in just the last three years. This number is increasing
                                                        further due to COVID-19 and the associated automation of education and testing.
                                                        Despite such wide usage, the AI based testing literature of these ‘intelligent’ models
                                                        is highly lacking. Most of the papers proposing new models rely only on qudratic
                                                        weighted kappa (QWK) based agreement with human raters for showing model
                                                        efficacy. However, this effectively ignores the highly multi-feature nature of essay
                                                        scoring. Essay scoring depends on features like coherence, grammar, relevance,
                                                        sufficiency, vocabulary, etc., and till date, there has been no study testing Automated
                                                        Essay Scoring (AES) systems holistically on all these features. With this motivation,
                                                        we propose a model agnostic adversarial evaluation scheme and associated metrics
                                                        for AES systems to test their natural language understanding capabilities and
                                                        overall robustness. We evaluate the current state-of-the-art AES models using the
                                                        proposed scheme and report the results on five recent models. These models range
                                                        from feature-engineering based approaches to the latest deep learning algorithms.
                                                        We find that AES models are highly overstable such that even heavy modifications
                                                        (as much as 25%) with content unrelated to the topic of the questions does not
                                                        decrease the score produced by the models. On the other hand, unrelated content,
                                                        on average, increases the scores, thus showing that the models’ evaluation strategy
                                                        and rubrics should be reconsidered. We also ask 200 human raters to score both an
                                                        original and adversarial response to see if humans are able to detect differences
                                                        between the two and whether they agree with the scores assigned by autoscorers.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Singla, Y. K., Parekh, S., Singh, S., Li, J. J., Shah, R. R., & Chen, C. (2021). AES Are
                                                        Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. arXiv
                                                        preprint arXiv:2109.11728.</p>
                                                    <p><b>DOI</b>: arXiv:2109.11728</p>
                                                    <p><b>Abstract:</b> Deep-learning based Automatic Essay Scoring (AES) systems are being
                                                        actively used by states and language testing agencies alike to evaluate millions of
                                                        candidates for life-changing decisions ranging from college applications to visa approvals. However, little research has been put to understand and interpret the
                                                        black-box nature of deep-learning based scoring algorithms. Previous studies
                                                        indicate that scoring models can be easily fooled. In this paper, we explore the
                                                        reason behind their surprising adversarial brittleness. We utilize recent advances in
                                                        interpretability to find the extent to which features such as coherence, content,
                                                        vocabulary, and relevance are important for automated scoring mechanisms. We
                                                        use this to investigate the oversensitivity (i.e., large change in output score with a
                                                        little change in input essay content) and overstability (i.e., little change in output
                                                        scores with large changes in input essay content) of AES. Our results indicate that
                                                        autoscoring models, despite getting trained as “end-to-end” models with rich
                                                        contextual embeddings such as BERT, behave like bag-of-words models. A few
                                                        words determine the essay score without the requirement of any context making
                                                        the model largely overstable. This is in stark contrast to recent probing studies on
                                                        pre-trained representation learning models, which show that rich linguistic features
                                                        such as parts-of-speech and morphology are encoded by them. Further, we also find
                                                        that the models have learnt dataset biases, making them oversensitive. The
                                                        presence of a few words with high co-occurence with a certain score class makes
                                                        the model associate the essay sample with that score. This causes score changes in
                                                        ∼95% of samples with an addition of only a few words. To deal with these issues, we
                                                        propose detection-based protection models that can detect oversensitivity and
                                                        overstability causing samples with high accuracies. We find that our proposed
                                                        models are able to detect unusual attribution patterns and flag adversarial samples
                                                        successfully.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Singla, Y. K., Gupta, A., Bagga, S., Chen, C., Krishnamurthy, B., & Shah, R. R.
                                                        (2021). Speaker-Conditioned Hierarchical Modeling for Automated Speech Scoring.
                                                        arXiv preprint arXiv:2109.00928.</p>
                                                    <p><b>DOI:</b> 10.1145/3459637.3482395</p>
                                                    <p><b>Abstract:</b> Automatic Speech Scoring (ASS) is the computer-assisted evaluation of a
                                                        candidate’s speaking proficiency in a language. ASS systems face many challenges
                                                        like open grammar, variable pronunciations, and unstructured or semi-structured
                                                        content. Recent deep learning approaches have shown some promise in this
                                                        domain. However, most of these approaches focus on extracting features from a
                                                        single audio, making them suffer from the lack of speaker-specific context required
                                                        to model such a complex task. We propose a novel deep learning technique for
                                                        non-native ASS, called speaker-conditioned hierarchical modeling. In our technique,
                                                        we take advantage of the fact that oral proficiency tests rate multiple responses for
                                                        a candidate. We extract context vectors from these responses and feed them as
                                                        additional speaker-specific context to our network to score a particular response.
                                                        We compare our technique with strong baselines and find that such modeling
                                                        improves the model’s average performance by 6.92% (maximum = 12.86%,
                                                        minimum = 4.51%). We further show both quantitative and qualitative insights into
                                                        the importance of this additional context in solving the problem of ASS.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Goyal, M., Goel, P., Mathur, P., & Shah, R. (2021, August).
                                                        Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task,
                                                        Dataset, and Neural Baselines. In Proceedings of the 59th Annual Meeting of the
                                                        Association for Computational Linguistics and the 11th International Joint Conference
                                                        on Natural Language Processing (Volume 1: Long Papers) (pp. 6751-6762).</p>
                                                    <p><b>DOI:</b> 10.18653/v1/2021.acl-long.526</p>
                                                    <p><b>Abstract:</b> Risk prediction is an essential task in financial markets. Merger and
                                                        Acquisition (M&A) calls provide key insights into the claims made by company
                                                        executives about the restructuring of the financial firms. Extracting vocal and
                                                        textual cues from M&A calls can help model the risk associated with such financial
                                                        activities. To aid the analysis of M&A calls, we curate a dataset of conference call
                                                        transcripts and their corresponding audio recordings for the time period ranging
                                                        from 2016 to 2020. We introduce M3ANet, a baseline architecture that takes
                                                        advantage of the multimodal multi-speaker input to forecast the financial risk
                                                        associated with the M&A calls. Empirical results prove that the task is challenging,
                                                        with the pro-posed architecture performing marginally better than strong
                                                        BERT-based baselines. We release the M3A dataset and benchmark models to
                                                        motivate future research on this challenging problem domain.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Ramit Sawhney, Shivam Agarwal, Megh Thakkar, Arnav Wadhwa, and Rajiv
                                                        Ratn Shah. 2021. Hyperbolic Online Time Stream Modeling. In <i>Proceedings of
                                                        the 44th International ACM SIGIR Conference on Research and Development in
                                                        Information Retrieval</i> (<i>SIGIR '21</i>). Association for Computing
                                                        Machinery, New York, NY, USA, 1682–1686.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3404835.3463119</p>
                                                    <p><b>Abstract:</b> The rapidly rising ubiquity and dissemination of online information such
                                                        as social media text and news improve user accessibility towards financial markets,
                                                        however, modeling these vast streams of irregular, temporal data poses a challenge.
                                                        Such temporal streams of information show power-law dynamics, scale-free
                                                        characteristics, and time irregularities that sequential models are unable to
                                                        accurately model. In this work, we propose the first Hierarchical Time-Aware
                                                        Hyperbolic LSTM (HTLSTM), which leverages the Riemannian manifold for
                                                        encoding the scale-free nature of a sequence of text in a time-aware fashion.
                                                        Through experiments on three financial tasks: stock trading, equity price movement
                                                        prediction, and financial risk prediction, we demonstrate HTLSTM's applicability for
                                                        modeling temporal sequences of online information. On real-world data from four
                                                        global stock markets and three stock indices spanning data in English and Chinese,
                                                        we make a step towards time-aware text modeling via hyperbolic geometry.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Agrawal, M., Mehrotra, P., Kumar, R., & Shah, R. R. (2021). Defending
                                                        Touch-based Continuous Authentication Systems from Active Adversaries Using
                                                        Generative Adversarial Networks. arXiv preprint arXiv:2106.07867.</p>
                                                    <p><b>Link:</b> https://arxiv.org/abs/2106.07867</p>
                                                    <p><b>Abstract:</b> Previous studies have demonstrated that commonly studied (vanilla)
                                                        touch-based continuous authentication systems (V-TCAS) are susceptible to
                                                        population attack. This paper proposes a novel Generative Adversarial Network
                                                        assisted TCAS (G-TCAS) framework, which showed more resilience to the
                                                        population attack. G-TCAS framework was tested on a dataset of 117 users who
                                                        interacted with a smartphone and tablet pair. On average, the increase in the false
                                                        accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%) for the
                                                        smartphone. Likewise, the increase in the FARs for V-TCAS was 25% compared to
                                                        G-TCAS (6%) for the tablet.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> S. Chopra, P. Mathur, R. Sawhney and R. R. Shah, "Meta-Learning for
                                                        Low-Resource Speech Emotion Recognition," ICASSP 2021 - 2021 IEEE
                                                        International Conference on Acoustics, Speech and Signal Processing (ICASSP),
                                                        2021, pp. 6259-6263</p>
                                                    <p><b>DOI:</b> 10.1109/ICASSP39728.2021.9414373.</p>
                                                    <p><b>Abstract:</b> While emotion recognition is a well-studied task, it remains unexplored
                                                        to a large extent in cross-lingual settings. Speech Emotion Recognition (SER) in
                                                        low-resource languages poses difficulties as existing approaches for knowledge
                                                        transfer do not generalize seamlessly. Probing the learning process of generalized
                                                        representations across languages, we propose a meta-learning approach for
                                                        low-resource speech emotion recognition. The proposed approach achieves fast
                                                        adaptation on a number of unseen target languages simultaneously. We evaluate the
                                                        Model Agnostic Meta-Learning (MAML) algorithm on three low-resource target
                                                        languages -Persian, Italian, and Urdu. We empirically demonstrate that our
                                                        proposed method - MetaSER 1 , considerably outperforms multitask and transfer
                                                        learning-based methods for speech emotion recognition task, and discuss the
                                                        benefits, efficiency, and challenges of MetaSER on limited data settings.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> A. N. Mathur, D. Batra, Y. K. Singla, R. Ratn Shah, C. Chen and R. Zimmermann,
                                                        "LIFI: Towards Linguistically Informed Frame Interpolation," ICASSP 2021 - 2021
                                                        IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
                                                        2021, pp. 7593-7597,</p>
                                                    <p><b>DOI:</b> 10.1109/ICASSP39728.2021.9413998.</p>
                                                    <p><b>Abstract:</b> Here we explore the problem of speech video interpolation. With close to
                                                        70% of web traffic, such content today forms the primary form of online
                                                        communication and entertainment. Despite high performance on conventional
                                                        metrics like MSE, PSNR, and SSIM, we find that the state-of-the-art frame
                                                        interpolation models fail to produce faithful speech interpolation. For instance, we
                                                        observe the lips stay static while the person is still speaking for most interpolated frames. With this motivation, using the information of words, sub-words, and
                                                        visemes, we provide a new set of linguistically informed metrics targeted explicitly
                                                        to the problem of speech video interpolation. We release several datasets to test
                                                        video interpolation models of their speech understanding. We also design
                                                        linguistically informed deep learning video interpolation algorithms to generate the
                                                        missing frames.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Wadhwa, A., Agarwal, S., & Shah, R. (2021, June). Quantitative
                                                        Day Trading from Natural Language using Reinforcement Learning. In Proceedings
                                                        of the 2021 Conference of the North American Chapter of the Association for
                                                        Computational Linguistics: Human Language Technologies (pp. 4018-4030).</p>
                                                    <p><b>DOI:</b> 10.18653/v1/2021.naacl-main.316</p>
                                                    <p><b>Abstract:</b> It is challenging to design profitable and practical trading strategies, as
                                                        stock price movements are highly stochastic, and the market is heavily influenced
                                                        by chaotic data across sources like news and social media. Existing NLP approaches
                                                        largely treat stock prediction as a classification or regression problem and are not
                                                        optimized to make profitable investment decisions. Further, they do not model the
                                                        temporal dynamics of large volumes of diversely influential text to which the
                                                        market responds quickly. Building on these shortcomings, we propose a deep
                                                        reinforcement learning approach that makes time-aware decisions to trade stocks
                                                        while optimizing profit using textual data. Our method outperforms state-ofthe-art
                                                        in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets
                                                        (English) and financial news (Chinese) pertaining to two major indexes and four
                                                        global stock markets. Through extensive experiments and studies, we build the case
                                                        for our method as a tool for quantitative trading.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Aggarwal, A., & Shah, R. (2021, June). An Empirical Investigation
                                                        of Bias in the Multimodal Analysis of Financial Earnings Calls. In Proceedings of the
                                                        2021 Conference of the North American Chapter of the Association for Computational
                                                        Linguistics: Human Language Technologies (pp. 3751-3757).</p>
                                                    <p><b>Link:</b> 10.18653/v1/2021.naacl-main.294</p>
                                                    <p><b>Abstract:</b> Volatility prediction is complex due to the stock market’s stochastic
                                                        nature. Existing research focuses on the textual elements of financial disclosures
                                                        like earnings calls transcripts to forecast stock volatility and risk, but ignores the
                                                        rich acoustic features in the company executives’ speech. Recently, new multimodal
                                                        approaches that leverage the verbal and vocal cues of speakers in financial
                                                        disclosures significantly outperform previous state-of-the-art approaches
                                                        demonstrating the benefits of multimodality and speech. However, the financial
                                                        realm is still plagued with a severe underrepresentation of various communities
                                                        spanning diverse demographics, gender, and native speech. While multimodal
                                                        models are better risk forecasters, it is imperative to also investigate the potential
                                                        bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal
                                                        volatility prediction due to gender sensitive audio features and fewer female
                                                        executives in earnings calls of one of the world’s biggest stock indexes, the S&P 500
                                                        index. We quantitatively analyze bias as error disparity and investigate the sources
                                                        of this bias. Our results suggest that multimodal neural financial models accentuate
                                                        gender-based stereotypes.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Joshi, H., Shah, R., & Flek, L. (2021, June). Suicide Ideation
                                                        Detection via Social and Temporal User Representations using Hyperbolic Learning.
                                                        In Proceedings of the 2021 Conference of the North American Chapter of the
                                                        Association for Computational Linguistics: Human Language Technologies (pp.
                                                        2176-2190).</p>
                                                    <p><b>Link:</b> https://aclanthology.org/2021.naacl-main.176</p>
                                                    <p><b>Abstract:</b> Recent psychological studies indicate that individuals exhibiting suicidal
                                                        ideation increasingly turn to social media rather than mental health practitioners.
                                                        Personally contextualizing the buildup of such ideation is critical for accurate
                                                        identification of users at risk. In this work, we propose a framework jointly
                                                        leveraging a user’s emotional history and social information from a user’s
                                                        neighborhood in a network to contextualize the interpretation of the latest tweet of
                                                        a user on Twitter. Reflecting upon the scale-free nature of social network
                                                        relationships, we propose the use of Hyperbolic Graph Convolution Networks, in
                                                        combination with the Hawkes process to learn the historical emotional spectrum of
                                                        a user in a timesensitive manner. Our system significantly outperforms
                                                        state-of-the-art methods on this task, showing the benefits of both socially and
                                                        personally contextualized representations.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Mathur, P., Jain, T., Gautam, A. K., & Shah, R. R. Multitask Learning
                                                        for Emotionally Analyzing Sexual Abuse Disclosures.</p>
                                                    <p><b>Link:</b> 10.18653/v1/2021.naacl-main.387</p>
                                                    <p><b>Abstract:</b> The #MeToo movement on social media platforms initiated discussions
                                                        over several facets of sexual harassment in our society. Prior work by the NLP
                                                        community for automated identification of the narratives related to sexual abuse
                                                        disclosures barely explored this social phenomenon as an independent task.
                                                        However, emotional attributes associated with textual conversations related to the
                                                        #MeToo social movement are complexly intertwined with such narratives. We
                                                        formulate the task of identifying narratives related to the sexual abuse disclosures
                                                        in online posts as a joint modeling task that leverages their emotional attributes
                                                        through multitask learning. Our results demonstrate that positive knowledge
                                                        transfer via context-specific shared representations of a flexible cross-stitched
                                                        parameter sharing model helps establish the inherent benefit of jointly modeling
                                                        tasks related to sexual abuse disclosures with emotion classification from the text in
                                                        homogeneous and heterogeneous settings. We show how for more domain-specific tasks related to sexual abuse disclosures such as sarcasm identification and
                                                        dialogue act (refutation, justification, allegation) classification, homogeneous
                                                        multitask learning is helpful, whereas for more general tasks such as stance and
                                                        hate speech detection, heterogeneous multitask learning with emotion
                                                        classification works better.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Joshi, H., Nobles, A., & Shah, R. R. (2021). Towards Emotion- and
                                                        Time-Aware Classification of Tweets to Assist Human Moderation for Suicide
                                                        Prevention. Proceedings of the International AAAI Conference on Web and Social
                                                        Media, 15(1), 609-620.</p>
                                                    <p><b>Link:</b> https://ojs.aaai.org/index.php/ICWSM/article/view/18088</p>
                                                    <p><b>Abstract:</b> Social media platforms are already engaged in leveraging existing online
                                                        socio-technical systems to employ just-in-time interventions for suicide prevention
                                                        to the public. These efforts primarily rely on self-reports of potential self-harm
                                                        content that is reviewed by moderators. Most recently, platforms have employed
                                                        automated models to identify self-harm content, but acknowledge that these
                                                        automated models still struggle to understand the nuance of human language (e.g.,
                                                        sarcasm). By explicitly focusing on Twitter posts that could easily be misidentified
                                                        by a model as expressing suicidal intent (i.e., they contain similar phrases such as
                                                        ``wanting to die''), our work examines the temporal differences in historical
                                                        expressions of general and emotional language prior to a clear expression of
                                                        suicidal intent. Additionally, we analyze time-aware neural models that build on
                                                        these language variants and factors in the historical, emotional spectrum of a user's
                                                        tweeting activity. The strongest model achieves high (statistically significant)
                                                        performance (macro F1=0.804, recall=0.813) to identify social media indicative of
                                                        suicidal intent. Using three use cases of tweets with phrases common to suicidal
                                                        intent, we qualitatively analyze and interpret how such models decided if suicidal
                                                        intent was present and discuss how these analyses may be used to alleviate the
                                                        burden on human moderators within the known constraints of how moderation is
                                                        performed (e.g., no access to the user's timeline). Finally, we discuss the ethical
                                                        implications of such data-driven models and inferences about suicidal intent from
                                                        social media. Content warning: this article discusses self-harm and suicide.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Agarwal, S., Wadhwa, A., Derr, T., & Shah, R. R. (2021, May). Stock
                                                        Selection via Spatiotemporal Hypergraph Attention Network: A Learning to Rank
                                                        Approach. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35,
                                                        No. 1, pp. 497-504).</p>
                                                    <p><b>Link:</b> https://www.aaai.org/AAAI21Papers/AAAI-7907.SawhneyR.pdf</p>
                                                    <p><b>Abstract:</b> Quantitative trading and investment decision making are intricate
                                                        financial tasks that rely on accurate stock selection. Despite advances in deep
                                                        learning that have made significant progress in the complex and highly stochastic
                                                        stock prediction problem, modern solutions face two significant limitations. They do not directly optimize the target of investment in terms of profit, and treat each
                                                        stock as independent from the others, ignoring the rich signals between related
                                                        stocks’ temporal price movements. Building on these limitations, we reformulate
                                                        stock prediction as a learning to rank problem and propose STHAN-SR, a neural
                                                        hypergraph architecture for stock selection. The key novelty of our work is the
                                                        proposal of modeling the complex relations between stocks through a hypergraph
                                                        and a temporal Hawkes attention mechanism to tailor a new spatiotemporal
                                                        attention hypergraph network architecture to rank stocks based on profit by jointly
                                                        modeling stock interdependence and the temporal evolution of their prices.
                                                        Through experiments on three markets spanning over six years of data, we show
                                                        that STHAN-SR significantly outperforms state-of-the-art neural stock forecasting
                                                        methods. We validate our design choices through ablative and exploratory analyses
                                                        over STHAN-SR’s spatial and temporal components and demonstrate its practical
                                                        applicability</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Yin, Y., Shrivastava, H., Zhang, Y., Liu, Z., Shah, R. R., & Zimmermann, R. (2021,
                                                        May). Enhanced Audio Tagging via Multi-to Single-Modal Teacher-Student Mutual
                                                        Learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No.
                                                        12, pp. 10709-10717).</p>
                                                    <p><b>Link:</b> https://www.aaai.org/AAAI21Papers/AAAI-920.YinY.pdf</p>
                                                    <p><b>Abstract:</b> Recognizing ongoing events based on acoustic clues has been a critical
                                                        yet challenging problem that has attracted significant research attention in recent
                                                        years. Joint audio-visual analysis can improve the event detection accuracy but may
                                                        not always be feasible as under many circumstances only audio recordings are
                                                        available in real-world scenarios. To solve the challenges, we present a novel
                                                        visual-assisted teacherstudent mutual learning framework for robust sound event
                                                        detection from audio recordings. Our model adopts a multimodal teacher network
                                                        based on both acoustic and visual clues, and a single-modal student network based
                                                        on acoustic clues only. Conventional teacher-student learning performs
                                                        unsatisfactorily for knowledge transfer from a multi-modality network to a
                                                        single-modality network. We thus present a mutual learning framework by
                                                        introducing a single-modal transfer loss and a cross-modal transfer loss to
                                                        collaboratively learn the audio-visual correlations between the two networks. Our
                                                        proposed solution takes the advantages of joint audiovisual analysis in training
                                                        while maximizing the feasibility of the model in use cases. Our extensive
                                                        experiments on the DCASE17 and the DCASE18 sound event detection datasets
                                                        show that our proposed method outperforms the state-of-the art audio tagging
                                                        approaches.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Vidit Jain, Maitree Leekha, Rajiv Ratn Shah, and Jainendra Shukla. 2021.
                                                        Exploring Semi-Supervised Learning for Predicting Listener Backchannels.
                                                        <i>Proceedings of the 2021 CHI Conference on Human Factors in Computing
                                                        Systems</i>. Association for Computing Machinery, New York, NY, USA, Article 395,
                                                        1–12.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3411764.3445449</p>
                                                    <p><b>Abstract:</b> Developing human-like conversational agentsis a prime area in HCI
                                                        research and subsumes many tasks. Predicting listener backchannels is one such
                                                        actively-researched task. While many studies have used different approaches for
                                                        backchannel prediction, they all have depended on manual annotations for a large
                                                        dataset. This is a bottleneck impacting the scalability of development. To this end,
                                                        we propose using semi-supervised techniques to automate the process of
                                                        identifying backchannels, thereby easing the annotation process. To analyze our
                                                        identification module’s feasibility, we compared the backchannel prediction models
                                                        trained on (a) manually-annotated and (b) semi-supervised labels. Quantitative
                                                        analysis revealed that the proposed semi-supervised approach could attain 95% of
                                                        the former’s performance. Our user-study fndings revealed that almost 60% of the
                                                        participants found the backchannel responses predicted by the proposed model
                                                        more natural. Finally, we also analyzed the impact of personality on the type of
                                                        backchannel signals and validated our fndings in the user-study.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Shah. 2021.
                                                        Exploring the Scale-Free Nature of Stock Markets: Hyperbolic Graph Learning for
                                                        Algorithmic Trading. In <i>Proceedings of the Web Conference 2021</i> (<i>WWW
                                                        '21</i>). Association for Computing Machinery, New York, NY, USA, 11–22.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3442381.3450095</p>
                                                    <p><b>Abstract:</b> Quantitative trading and investment decision making are intricate
                                                        financial tasks in the ever-increasing sixty trillion dollars global stock market.
                                                        Despite advances in stock forecasting, a limitation of most existing neural methods
                                                        is that they treat stocks independent of each other, ignoring the valuable rich signals
                                                        between related stocks’ movements. Motivated by financial literature that shows
                                                        stock markets and inter-stock correlations show scale-free network characteristics,
                                                        we leverage domain knowledge on the Web to model inter-stock relations as a
                                                        graph in four major global stock markets and formulate stock selection as a
                                                        scale-free graph-based learning to rank problem. To capture the scale-free spatial
                                                        and temporal dependencies in stock prices, we propose HyperStockGAT: Hyperbolic
                                                        Stock Graph Attention Network, the first model on the Riemannian Manifolds for
                                                        stock selection. Our work’s key novelty is the proposal of modeling the complex,
                                                        scale-free nature of inter-stock relations through temporal hyperbolic graph
                                                        learning on Riemannian manifolds that can represent the spatial correlations
                                                        between stocks more accurately. Through extensive experiments on long-term
                                                        real-world data spanning over six years on four of the world’s biggest markets:
                                                        NASDAQ, NYSE, TSE, and China exchanges, we show that HyperStockGAT
                                                        significantly outperforms state-of-the-art stock forecasting methods in terms of
                                                        profitability by over 12%, and risk-adjusted Sharpe Ratio by over 4%. We analyze
                                                        HyperStockGAT’s components’ contributions through a series of exploratory and
                                                        ablative experiments to demonstrate its practical applicability to real-world trading. Furthermore, we propose a novel hyperbolic architecture that can be applied across
                                                        various spatiotemporal problems on the Web’s commonly occurring scale-free
                                                        networks.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Joshi, H., Gandhi, S., & Shah, R. R. (2021, March). Towards Ordinal
                                                        Suicide Ideation Detection on Social Media. In Proceedings of the 14th ACM
                                                        International Conference on Web Search and Data Mining (pp. 22-30).</p>
                                                    <p><b>Link:</b> https://doi.org/10.1145/3437963.3441805</p>
                                                    <p><b>Abstract:</b> The rising ubiquity of social media presents a platform for individuals to
                                                        express suicide ideation, instead of traditional, formal clinical settings. While neural
                                                        methods for assessing suicide risk on social media have shown promise, a crippling
                                                        limitation of existing solutions is that they ignore the inherent ordinal nature across
                                                        finegrain levels of suicide risk. To this end, we reformulate suicide risk assessment
                                                        as an Ordinal Regression problem, over the ColumbiaSuicide Severity Scale. We
                                                        propose SISMO, a hierarchical attention model optimized to factor in the graded
                                                        nature of increasing suicide risk levels, through soft probability distribution since
                                                        not all wrong risk-levels are equally wrong. We establish the face value of SISMO for
                                                        preliminary suicide risk assessment on real-world Reddit data annotated by clinical
                                                        experts. We conclude by discussing the empirical, practical, and ethical
                                                        considerations pertaining to SISMO in a larger picture, as a human-in-the-loop
                                                        framework.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Mehnaz, L., Mahata, D., Gosangi, R., Gunturi, U. S., Jain, R., Gupta, G., ... & Shah,
                                                        R. R. (2021). GupShup: An Annotated Corpus for Abstractive Summarization of
                                                        Open-Domain Code-Switched Conversations. arXiv preprint arXiv:2104.08578.</p>
                                                    <p><b>Link:</b> arXiv preprint arXiv:2104.08578</p>
                                                    <p><b>Abstract:</b> Code-switching is the communication phenomenon where speakers
                                                        switch between different languages during a conversation. With the widespread
                                                        adoption of conversational agents and chat platforms, code-switching has become
                                                        an integral part of written conversations in many multi-lingual communities
                                                        worldwide. This makes it essential to develop techniques for summarizing and
                                                        understanding these conversations. Towards this objective, we introduce
                                                        abstractive summarization of Hindi-English code-switched conversations and
                                                        develop the first code-switched conversation summarization dataset - GupShup,
                                                        which contains over 6,831 conversations in Hindi-English and their corresponding
                                                        human annotated summaries in English and Hindi-English. We present a detailed
                                                        account of the entire data collection and annotation processes. We analyze the
                                                        dataset using various code-switching statistics. We train stateof-the-art abstractive
                                                        summarization models and report their performances using both automated
                                                        metrics and human evaluation. Our results show that multi-lingual mBART and
                                                        multi-view seq2seq models obtain the best performances on the new dataset1 .</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Sawhney, R., Wadhwa, A., Agarwal, S., & Shah, R. (2021, April). FAST: Financial
                                                        News and Tweet Based Time Aware Network for Stock Trading. In Proceedings of
                                                        the 16th Conference of the European Chapter of the Association for Computational
                                                        Linguistics: Main Volume (pp. 2164-2175).</p>
                                                    <p><b>Link:</b> https://aclanthology.org/2021.eacl-main.185.pdf</p>
                                                    <p><b>Abstract:</b> Designing profitable trading strategies is complex as stock movements
                                                        are highly stochastic; the market is influenced by large volumes of noisy data across
                                                        diverse information sources like news and social media. Prior work mostly treats
                                                        stock movement prediction as a regression or classification task and is not directly
                                                        optimized towards profit-making. Further, they do not model the fine-grain
                                                        temporal irregularities in the release of vast volumes of text that the market
                                                        responds to quickly. Building on these limitations, we propose a novel hierarchical,
                                                        learning to rank approach that uses textual data to make time-aware predictions for
                                                        ranking stocks based on expected profit. Our approach outperforms state-of-the-art
                                                        methods by over 8% in terms of cumulative profit and risk-adjusted returns in
                                                        trading simulations on two benchmarks: English tweets and Chinese financial news
                                                        spanning two major stock indexes and four global markets. Through ablative and
                                                        qualitative analyses, we build the case for our method as a tool for daily stock
                                                        trading.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Kashyap, A. R., Mehnaz, L., Malik, B., Waheed, A., Hazarika, D., Kan, M. Y., &
                                                        Shah, R. (2021, April). Analyzing the Domain Robustness of Pretrained Language
                                                        Models, Layer by Layer. In Proceedings of the Second Workshop on Domain
                                                        Adaptation for NLP (pp. 222-244).</p>
                                                    <p><b>DOI:</b> https://aclanthology.org/2021.adaptnlp-1.23/</p>
                                                    <p><b>Abstract:</b> The robustness of pretrained language models(PLMs) is generally
                                                        measured using performance drops on two or more domains. However, we do not
                                                        yet understand the inherent robustness achieved by contributions from different
                                                        layers of a PLM. We systematically analyze the robustness of these representations
                                                        layer by layer from two perspectives. First, we measure the robustness of
                                                        representations by using domain divergence between two domains. We find that i)
                                                        Domain variance increases from the lower to the upper layers for vanilla PLMs; ii)
                                                        Models continuously pretrained on domain-specific data (DAPT)(Gururangan et al.,
                                                        2020) exhibit more variance than their pretrained PLM counterparts; and that iii)
                                                        Distilled models (e.g., DistilBERT) also show greater domain variance. Second, we
                                                        investigate the robustness of representations by analyzing the encoded syntactic
                                                        and semantic information using diagnostic probes. We find that similar layers have
                                                        similar amounts of linguistic information for data from an unseen domain.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Yifang Yin, Ying Zhang, Zhenguang Liu, Sheng Wang, Rajiv Ratn Shah, and
                                                        Roger Zimmermann. "GPS2Vec: Pre-trained Semantic Embeddings for Worldwide
                                                        GPS Coordinates." IEEE Transactions on Multimedia (2021).</p>
                                                    <p><b>DOI:</b> 10.1109/TMM.2021.3060951</p>
                                                    <p><b>Abstract:</b> GPS coordinates are fine-grained location indicators that are difficult to
                                                        be effectively utilized by classifiers in geo-aware applications. Previous GPS
                                                        encoding methods concentrate on generating hand-crafted features for small areas
                                                        of interest. However, many real world applications require a machine learning
                                                        model, analogous to the pre-trained ImageNet model for images, that can efficiently
                                                        generate semantically-enriched features for planet-scale GPS coordinates. To
                                                        address this issue, we propose a novel two-level grid-based framework, termed
                                                        GPS2Vec, which is able to extract geo-aware features in real-time for locations
                                                        worldwide. The Earth's surface is first discretized by the Universal Transverse
                                                        Mercator (UTM) coordinate system. Each UTM zone is then considered as a local
                                                        area of interest that is further divided into fine-grained cells to perform the initial
                                                        GPS encoding. We train a neural network in each UTM zone to learn the semantic
                                                        embeddings from the initial GPS encoding. The training labels can be automatically
                                                        derived from large-scale geotagged documents such as tweets, check-ins, and
                                                        images that are available from social sharing platforms. We conducted
                                                        comprehensive experiments on three geo-aware applications, namely place
                                                        semantic annotation, geotagged image classification, and next location prediction.
                                                        Experimental results demonstrate the effectiveness of our approach, as prediction
                                                        accuracy improves significantly based on a simple multi-feature early fusion
                                                        strategy with deep neural networks, including both CNNs and RNNs.</p>
                                                </li> 
                                                <li>
                                                    <p><b>Title:</b> Ramit Sawhney, Harshit Joshi, Saumya Gandhi, Di Jin, Rajiv Ratn Shah, Robust
                                                        suicide risk assessment on social media via deep adversarial learning, Journal of the
                                                        American Medical Informatics Association, Volume 28, Issue 7, July 2021, Pages
                                                        1497–1506</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1093/jamia/ocab031</p>
                                                    <p><b>Abstract:</b> The prevalence of social media for sharing personal thoughts makes it a
                                                        viable platform for the assessment of suicide risk. However, deep learning models
                                                        are not able to capture the diverse nature of linguistic choices and temporal
                                                        patterns that can be exhibited by a suicidal user on social media and end up
                                                        overfitting on specific cues that are not generally applicable. We propose
                                                        Adversarial Suicide assessment Hierarchical Attention (ASHA), a hierarchical
                                                        attention model that employs adversarial learning for improving the generalization
                                                        ability of the model.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Deepak Sharma, Bijendra Kumar, Satish Chand, and Rajiv Ratn Shah.
                                                        "Uncovering research trends and topics of communities in machine learning."
                                                        Multimedia Tools and Applications 80, no. 6 (2021): 9281-9314.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1007/s11042-020-10072-8</p>
                                                    <p><b>Abstract:</b> This paper aims to uncover the research topics in machine learning
                                                        research communities in a scientific collaboration network (SCN) to enhance the
                                                        characteristic of systems such as retrieval or recommendation in intelligence-based
                                                        systems. The existing research mainly focuses on the community evolution and
                                                        measurement of typical features of the network. It is however unexplored how to
                                                        identify the research interest of the communities along with authors in each
                                                        community. A dataset is prepared consisting of 21,906 scientific articles from six
                                                        top journals in the field of machine learning published from 1988 to 2017. An
                                                        integrated approach combining the author-topic (AT) model with communities
                                                        using through the directed affiliations (CoDA) method is explored to identify the
                                                        research interest of the communities in a scientific collaboration network. The top
                                                        rank communities are identified using the crank network community prioritization
                                                        method. Finally, the similarity and dissimilarity of research interest in communities
                                                        across decades are uncovered using the cosine similarity. The experimental results
                                                        demonstrate the effectiveness and efficacy of the proposed technique. This study
                                                        may be helpful for upcoming researchers to explore the research trends and topics
                                                        in machine learning research communities.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Shagun Uppal, Anish Madan, Sarthak Bhagat, Yi Yu, and Rajiv Ratn Shah. 2021.
                                                        C3VQG: category consistent cyclic visual question generation. In Proceedings of the
                                                        2nd ACM International Conference on Multimedia in Asia (MMAsia '20). Association
                                                        for Computing Machinery, New York, NY, USA, Article 49, 1–7.</p>
                                                    <p><b>DOI:</b> https://doi.org/10.1145/3444685.3446302</p>
                                                    <p><b>Abstract:</b> Visual Question Generation (VQG) is the task of generating natural
                                                        questions based on an image. Popular methods in the past have explored
                                                        image-to-sequence architectures trained with maximum likelihood which have
                                                        demonstrated meaningful generated questions given an image and its associated
                                                        ground-truth answer. VQG becomes more challenging if the image contains rich
                                                        contextual information describing its different semantic categories. In this paper,
                                                        we try to exploit the different visual cues and concepts in an image to generate
                                                        questions using a variational autoencoder (VAE) without ground-truth answers.
                                                        Our approach solves two major shortcomings of existing VQG systems: (i) minimize
                                                        the level of supervision and (ii) replace generic questions with category relevant
                                                        generations. Most importantly, by eliminating expensive answer annotations, the
                                                        required supervision is weakened. Using different categories enables us to exploit
                                                        different concepts as the inference requires only the image and the category. Mutual
                                                        information is maximized between the image, question, and answer category in the
                                                        latent space of our VAE. A novel category consistent cyclic loss is proposed to enable
                                                        the model to generate consistent predictions with respect to the answer category, reducing redundancies and irregularities. Additionally, we also impose
                                                        supplementary constraints on the latent space of our generative model to provide
                                                        structure based on categories and enhance generalization by encapsulating
                                                        decorrelated features within each dimension. Through extensive experiments, the
                                                        proposed model, C3VQG outperforms state-of-the-art VQG methods with weak
                                                        supervision.</p>
                                                </li>
                                                
                                            </ol> 
                                            </div>
                                    </a>
                                </li>
                            
                                <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2019"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Dr. Venkata Ratnadeep Suri</h5>
                                                <!-- <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2019</span> -->
                                            </div>
                                            <ol class="text-gray gara-proreg" style="font-size: 1.2em;">
                                                <li>
                                                    <p><b>Title:</b> Fiona McGaughey, Richard Watermeyer, Kalpana Shankar, Venkata Ratnadeep
                                                        Suri, Cathryn Knight, Tom Crick, Joanne Hardman, Dean Phelan & Roger Chung
                                                        (2021) ‘This can’t be the new norm’: academics’ perspectives on the COVID-19 crisis
                                                        for the Australian university sector, Higher Education Research & Development</p>
                                                    <p><b>DOI:</b> 10.1080/07294360.2021.1973384</p>
                                                    <p><b>Abstract:</b> The COVID-19 pandemic has profoundly affected the university sector
                                                        globally. This article reports on the Australian findings from a large-scale survey of
                                                        academic staff and their experiences and predictions of the impact of the pandemic
                                                        on their wellbeing. We report the perceptions of n = 370 Australian academics and
                                                        accounts of their institutions’ responses to COVID-19, analysed using
                                                        self-determination theory. Respondents report work-related stress, digital fatigue,
                                                        and a negative impact on work-life balance; as well as significant concerns over
                                                        potential longer-term changes to academia as a result of the pandemic. Respondents
                                                        also articulate their frustration with Australia’s neoliberal policy architecture and
                                                        the myopia of quasi-market reform, which has spawned an excessive reliance on
                                                        international students as a pillar of income generation and therefore jeopardised
                                                        institutional solvency – particularly during the pandemic. Conversely, respondents
                                                        identify a number of ‘silver linings’ which speak to the resilience of academics.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Richard Watermeyer, Kalpana Shankar, Tom Crick, Cathryn Knight, Fiona
                                                        McGaughey, Joanna Hardman, Venkata Ratnadeep Suri, Roger Chung & Dean Phelan
                                                        (2021) ‘Pandemia’: a reckoning of UK universities’ corporate response to COVID-19
                                                        and its academic fallout, British Journal of Sociology of Education</p>
                                                    <p><b>DOI:</b> 10.1080/01425692.2021.1937058</p>
                                                    <p><b>Abstract:</b> Universities in the UK, and in other countries like Australia and the USA,
                                                        have responded to the operational and financial challenges presented by the
                                                        COVID-19 pandemic by prioritising institutional solvency and enforcing changes to
                                                        the work practices and profiles of their staff. For academics, an adjustment to
                                                        institutional life under COVID-19 has been dramatic and resulted in the
                                                        overwhelming majority making a transition to prolonged remote-working. Many
                                                        have endured significant work intensification; others have lost – or may soon lose – their jobs. The impact of the pandemic appears transformational and for the most
                                                        part negative. This article reports the experiences of 1099 UK academics specific to
                                                        the corporate response of institutional leadership to the COVID-19 crisis. We find
                                                        articulated a story of universities in the grip of ‘pandemia’ and COVID-19
                                                        emboldening processes and protagonists of neoliberal governmentality and market
                                                        reform that pay little heed to considerations of human health and well-being.</p>
                                                </li>
                                                <li>
                                                    <p><b>Title:</b> Kalpana Shankar, Dean Phelan, Venkata Ratnadeep Suri, Richard Watermeyer,
                                                        Cathryn Knight & Tom Crick (2021) ‘The COVID-19 crisis is not the core problem’:
                                                        experiences, challenges, and concerns of Irish academia during the pandemic, Irish
                                                        Educational Studies, 40:2, 169-175</p>
                                                    <p><b>DOI:</b> 10.1080/03323315.2021.1932550</p>
                                                    <p><b>Abstract:</b> This article, drawing on data from an international survey – distributed in
                                                        the summer of 2020 – explores the experiences and concerns of academic staff (n =
                                                        167) working in universities in Ireland and their perceptions of their institutions’
                                                        early response to the pandemic. Concerns related to transitioning to remote online
                                                        working, impact on research productivity and culture, and work intensification, as
                                                        intersected by enhanced managerialism, are ubiquitous to their accounts. As some
                                                        respondents wrote of potential positive changes, particularly in the delivery of
                                                        teaching, we conclude by suggesting potential avenues for building on successes in
                                                        coping with the pandemic with some recommendations for mitigating some of the
                                                        harms.</p>
                                                </li>
                                                
                                            </ol>
                                        </div>
                                    </a>
                                </li>
                            
                        
                            
                                <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2018"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Dr. Richa Gupta</h5>
                                                <!-- <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2018</span> -->
                                            </div>
                                               <ul class="text-gray gara-proreg" style="font-size: 1.2em;">
                                                <li>
                                                    <p><b>Profile:</b> Dr. Richa is the newest core member of the CDNM and joined on 15 July, 2021. She
                                                        completed Ph.D. from IIT Delhi in 2020. She holds a postgraduate degree in Industrial
                                                        Design (M.Des.) from IDC, IIT Bombay (2013) and completed B.Tech. in Mechanical
                                                        Engineering from IIIT Jabalpur (2011). She has done collaborative research at the School of
                                                        Informatics and Computing, IUPU Indianapolis, USA (2017-18) and TU Darmstadt,
                                                        Germany (2012). She has also worked as Project Scientist at AssisTech Labs, IIT Delhi
                                                        where she contributed in design and development of several award winning translational
                                                        research projects, namely SmartCane Device, DotBook (Braille Laptop for Blind), TacRead,
                                                        OnBoard Bus Identification System, Accessible Graphics Design, and Multi-Modal Braille
                                                        Learning Device. She is recipient of several prestigious fellowships namely, Stanford Ignite
                                                        Global Innovation Fellowship (2015), Visvesvaraya PhD Fellowship (2015), JENESYS
                                                        Fellowship (Indo-Japan Exchange Program, 2010). She was awarded the Chairman’s Silver
                                                        Medal for Excellence in Academics at IIIT Jabalpur in 2011.</p>
                                                    <p>Her current research interests are: Perceptual foundations of Design, Inclusive Design and
                                                        Accessibility, Product Design & Modern Prototyping, Multi-modal Interaction/Experience
                                                        Design</p>
                                                </li>
                                            
                                            </ul>                                        
                                            </div>
                                    </a>
                                </li>
                            
                                <!-- <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2018"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Dr. Rajiv Ratn Shah</h5>
                                                <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2018</span>
                                            </div>
                                            <ul class="text-gray gara-proreg" style="font-size: 1.2em;">
                                            <li>Yaman Kumar, Rohit Jain, Mohd Salik, Rajiv Ratn Shah, Roger Zimmermann, Yifang Yin, “MyLipper: A Personalized System for Speech Reconstruction using Multi-view Visual Feeds.” In IEEE ISM, 2018.</li>
                                            <li>Meghna Ayyar, Puneet Mathur, Rajiv Ratn Shah, Shree Gopal Sharma, “Harnessing AI for Kidney Glomeruli Classification.” In IEEE ISM, 2018.</li>
                                            <li>Yaman Kumar, Mayank Aggarwal, Pratham Nawal, Shinichi Satoh, Rajiv Ratn Shah, Roger Zimmerman, “Harnessing AI for Speech Reconstruction using Multi-view Silent Video Feed.” In ACM Multimedia, 2018.</li>
                                            <li>Debanjan Mahata, Jasper Friedrichs, Rajiv Ratn Shah, and Jing Jiang, “Detecting Personal Intake of Medicine from Twitter.” In IEEE Intelligent Systems on Affective Computing and Sentiment Analysis, 2018.</li>
                                            <li>Yifang Yin, Rajiv Ratn Shah, Roger Zimmerman, “Learning and Fusing Multimodal Deep Features for Acoustic Scene Categorization.” In ACM Multimedia, 2018.</li>
                                            <li>Debanjan Mahata, John Kuriakose, Rajiv Ratn Shah, Roger Zimmermann, “Key2Vec: Automatic Ranked Keyphrase Extraction from Scientific Articles using Phrase Embeddings.” In NAACL, 2018.</li>
                                            <li>Yifang Yin, Rajiv Ratn Shah, Guanfeng Wang, Roger Zimmermann, “Feature-based Map Matching for Low-Sampling-Rate GPS Trajectories.” In ACM Transactions on Spatial Algorithms and Systems, 2018.</li>
                                            <li>Yaman Kumar, Agniv Sharma, Abhigyan Khaund, Akash Kumar, Ponnurangam Kumaraguru, Rajiv Ratn Shah, Roger Zimmermann. “IceBreaker: Solving Cold Start Problem for Video Recommendation Engines.” In MR2AMC at ISM, 2018.</li>
                                            <li>Aditya Kumar Pathak, Himanshu Choudhary, Rajiv Ratn Shah, Ponnurangam Kumaraguru, “Neural Machine Translation for English-Tamil.” In WMT at EMNLP, 2018.</li>
                                            <li>Puneet Mathur, Ramit Sawhney, Rajiv Ratn Shah and Debanjan Mahata, “Did you offend me? Classification of Offensive Tweets in Hinglish Language.” In ALW2 at EMNLP, 2018.</li>
                                            <li>Ramit Sawhney, Prachi Manchanda, Puneet Mathur, Rajiv Ratn Shah, Raj Singh, Ramit Sawhney, “Exploring and Learning Suicidal Ideation Connotations on Social Media with Deep Learning.” In WASSA at EMNLP, 2018.</li>
                                            <li>Puneet Mathur, Meghna Ayyar, Sahil Chopra, Simra Shahid, Laiba Mehnaz and Rajiv Ratn Shah, “Identification of Emergency Blood Donation Request on Twitter.” In SMM4H at EMNLP, 2018.</li>
                                            <li>Puneet Mathur, Rajiv Ratn Shah, Ramit Sawhney, and Debanjan Mahata, “Detecting Offensive Tweets in Hindi-English Code-Switched Language.” In ACL, pp. 18-26, 2018.</li>
                                            <li>Sarthak Anand, Rajat Gupta, Rajiv Ratn Shah, Ponnurangam Kumaraguru, “Fully Automatic Approach to Identify Factual or Fact-checkable Tweets.” In FIRE working notes, 2018.</li>
                                            <li>Deepanshu Gaur, Meghna Ayyar, Ashutosh Kumar Singh, and Rajiv Ratn Shah, “Multilingual Author Profiling from SMS.” In FIRE working notes, 2018.</li>
                                            <li>Ayush Gupta, Meghna Ayyar, Ashutosh Kumar Singh, and Rajiv Ratn Shah, “Named Entity Recognition for Indian Languages.” In FIRE working notes, 2018.</li>
                                            <li>Aditya Kumar Pathak, Himanshu Choudhary, Rajiv Ratn Shah, “NMT based Tamil Translation.” In FIRE working notes, 2018.</li>
                                            </ul>
                                        </div>
                                    </a>
                                </li>
                            
                        
                            
                                <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2019"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h3 class="text-green font-weight-bold">Conferences</h3><br>
                                                <h5 class="text-green font-weight-bold">Mr. Rahul Mohanani</h5>
                                                <span class="p-1 border border-gray rounded text-gray" style="width: 165px;"> Held In: 2019 </span>
                                            </div>
                                            <p class="text-gray gara-proreg" style="font-size: 1.2em;">Becker, C., Fagerholm, F., Mohanani, R. and Chatzigeorgiou, A., 2019. Temporal Discounting in Technical Debt: How do Software Practitioners Discount the Future?.  In 2019 IEEE 2019 International Conference on Technical Debt, Montreal, Canada. IEEE. </p>
                                        </div>
                                    </a>
                                </li> -->
                            
                                <!-- <li>
                                    <a href="#" class="listing">
                                        <div id="Year_2017"  
                                        onmouseover="this.classList.add('shadow'); this.classList.remove('shadow-sm');"
                                        onmouseout="this.classList.remove('shadow'); this.classList.add('shadow-sm');"
                                        class="mt-4 p-3 shadow-sm">
                                            <div class="d-flex flex-column flex-md-row justify-content-between">
                                                <h5 class="text-green font-weight-bold">Mrinmoy Chakrabarty, Makoto Wada</h5>
                                                <span class="p-1 border border-gray rounded text-gray" style="width: 165px;">Publication Year: 2017</span>
                                            </div>
                                            <p class="text-gray gara-proreg" style="font-size: 1.2em;">&#34;Perceptual effects of fast and automatic visual ensemble statistics from faces in individuals with typical development and autism spectrum conditions&#34;, in Scientific Reports, February 2017 [Nature Publishing Group]. (Published)</p>
                                        </div>
                                    </a>
                                </li> -->
                            
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</div>

        <div id="footer" class="bg-black" style="clear: both;"> 
    <div class="container py-3 pt-4 text-white">
        <div class="row">
            <div class="col-12">
                <a href="/">
                    <img src="/static/images/HCDLOGO.png" style="height:60px;" alt="Human Centered Design">
                </a>
                <hr class="text-white bg-white">
            </div>
            <div class="col-12 col-md-3">
                <h5 class="mb-1 heading">Links</h5>
                <ul class="list-unstyled mt-2">
                    <li class="py-1"><a rel="noopener noreferrer" class="text-center text-white medium" href="https://www.iiitd.ac.in/admission">Admissions</a></li>
                    <li class="py-1"><a rel="noopener noreferrer" class="text-center text-white medium" href="https://www.iiitd.ac.in/placement">Placements</a></li>
                    <li class="py-1"><a rel="noopener noreferrer" class="text-center text-white medium" href="https://iiitd.ac.in/">IIIT-Delhi</a></li>
                </ul>
            </div>
            <div class="col-12 col-md-3">
                <h5 class="mb-1 mt-3 mt-md-0 heading">Follow Us</h5>
                <ul class="d-flex align-items-end flex-column mt-2 list-unstyled">
                    <li class="w-100">
                        <a rel='noopener noreferrer' href="https://twitter.com/hcd_IIITD" class="py-1 text-decoration-none" target="_blank">
                            <img alt="Human Centered Design - Twitter" src="/static/images/twitter_svg.svg" height="35">
                        </a>
                        <a rel='noopener noreferrer' href="https://www.facebook.com/hcdzine" class="ml-2 py-1 text-decoration-none" target="_blank">
                            <img alt="Human Centered Design - Facebook" src="/static/images/facebook_hcd.svg" height="35">
                        </a>
                        <a rel='noopener noreferrer' href="https://www.linkedin.com/in/human-centered-design-hcd-43b3a7180/" class="ml-2 py-1 text-decoration-none" target="_blank">
                            <img alt="Human Centered Design - LinkedIn" src="/static/images/linkedin_hcd.svg" height="35">
                        </a>
                        <a rel='noopener noreferrer' href="https://www.instagram.com/hcd.iiitd/" class="ml-2 py-1 text-decoration-none" target="_blank">
                            <svg style="width: 30px; height: 30px;" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="instagram" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="text-green"><path fill="currentColor" d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z" class=""></path></svg>
                        </a>
                    </li>
                </ul>      
            </div> 
            <div class="col-12 col-md-3">
                <h5 class="mb-1 mt-3 mt-md-0 heading">Contacts</h5>
                <ul class="list-unstyled">
                    <li class="py-1"> 
                        <svg width="10" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="map-marker-alt" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="#ffffff" d="M172.268 501.67C26.97 291.031 0 269.413 0 192 0 85.961 85.961 0 192 0s192 85.961 192 192c0 77.413-26.97 99.031-172.268 309.67-9.535 13.774-29.93 13.773-39.464 0zM192 272c44.183 0 80-35.817 80-80s-35.817-80-80-80-80 35.817-80 80 35.817 80 80 80z" class=""></path></svg> &nbsp;
                        <a rel='noopener noreferrer' href="https://www.google.com/maps?ll=28.545628,77.273151&z=15&t=m&hl=en-US&gl=US&mapclient=embed&cid=3195354736735536470" class="text-white">Google Maps</a></li>
                    <li class="py-1">
                        <svg style="fill: white" xmlns="http://www.w3.org/2000/svg" width="10" viewBox="0 0 24 24"><path d="M12 12.713l-11.985-9.713h23.97l-11.985 9.713zm0 2.574l-12-9.725v15.438h24v-15.438l-12 9.725z"/></svg>
                        &nbsp;
                        <a rel='noopener noreferrer' href="mailto:admin-hcd@iiitd.ac.in" class="text-white">admin-hcd@iiitd.ac.in</a></li>
                    <li class="py-1">
                        <svg style="fill: white" xmlns="http://www.w3.org/2000/svg" width="14" viewBox="0 0 24 24"><path d="M20 22.621l-3.521-6.795c-.008.004-1.974.97-2.064 1.011-2.24 1.086-6.799-7.82-4.609-8.994l2.083-1.026-3.493-6.817-2.106 1.039c-7.202 3.755 4.233 25.982 11.6 22.615.121-.055 2.102-1.029 2.11-1.033z"/></svg>
                        <a rel='noopener noreferrer' href="tel:+91-11-26907588" class="text-white">+91-11-26907588</a>
                    </li>
                </ul>
            </div> 
            <div class="col-12 col-md-3">
                <ul class="list-unstyled mt-3 mt-md-0">
                    <li class="h5 text-left text-white medium">
                        Design and Development By
                    </li>
                    <li class="py-1">
                        <a rel="noopener noreferrer" class="text-center text-white medium" target="_blank" href="https://www.linkedin.com/in/manavjeet-singh-2172ba18b/">Manavjeet Singh</a>
                    </li>
                    <li class="py-1">
                        <a rel="noopener noreferrer" class="text-center text-white medium" target="_blank" href="https://www.linkedin.com/in/digitalplayer1125/">Rishi Raj Jain</a>
                    </li>
                </ul>
            </div>    
        </div>
        <div class="row justify-content-center">
            <div class="col">
                <p class="small text-white">Copyright © 2020 | All Rights Reserved</p>
            </div>
        </div>
    </div>
</div>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha384-vk5WoKIaW/vJyUAd9n/wmopsmNhiy+L2Z+SBxGYnUkunIxVxAv/UtMOhba/xskxh" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
        
<script>
    function myFunction() {
        var input, filter, ul, li, a, txtValue;
        input = document.getElementById("myInput");
        filter = input.value.toUpperCase();
        ul = document.getElementById("myUL");
        li = ul.getElementsByTagName("li");
        for (let i = 0; i < li.length; i++) {
            a = li[i].getElementsByTagName("a")[0];
            txtValue = a.textContent || a.innerText;
            if (txtValue.toUpperCase().indexOf(filter) > -1) {
                li[i].style.display = "";
            } else {
                li[i].style.display = "none";
            }
        }
    }
</script>

        <script src="/static/js/base.js"></script>
    </body>
</html>